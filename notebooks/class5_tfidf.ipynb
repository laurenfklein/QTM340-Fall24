{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDtvbdozFGPC"
      },
      "source": [
        "# Counting and Vectorizing Words with sk-learn\n",
        "\n",
        "We will get back to large language models very soon. But it remains true that in many cases, just counting words can tell you a lot. To wit:\n",
        "\n",
        "<img src=\"http://lklein.com/wp-content/uploads/2021/10/Screen-Shot-2021-10-06-at-3.33.34-PM.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, we're going to explore a measure called Term Frequency - Inverse Document Frequency (tf-idf). Tf-idf comes up a lot in text analysis projects because it’s both a corpus exploration method and a pre-processing step for many other text-mining measures and models.\n",
        "\n",
        "The procedure was introduced in a 1972 paper by Karen Spärck Jones under the name “term specificity,” and the basic idea is this:\n",
        "\n",
        "Instead of representing a term in a document by its raw frequency or its relative frequency (the term count divided by the document length), each term is *weighted* by dividing the term frequency by the number of documents in the corpus containing the word.\n",
        "\n",
        "The overall effect of this weighting scheme is to avoid a common problem when conducting text analysis: the most frequently used words in any particular document are often the most frequently used words in all of the documents. Therefore, they are not particularly informative!\n",
        "\n",
        "By contrast, terms with the highest tf-idf scores are the terms that are *distinctively* frequent in any particular document when that document is compared other documents. When you sort by tf-idf score, these distinctive terms rise to the top."
      ],
      "metadata": {
        "id": "y-WcWquZLqz1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rJiI3FjFGPL"
      },
      "source": [
        "## An Analogy ##\n",
        "    \n",
        "If this explanation doesn’t quite resonate, a brief analogy might help.\n",
        "\n",
        "Say you've decided leave campus to get dinner on Buford Highway. Since leaving campus takes a lot of effort (and also, crucially, access to a car), the food better be worth it! That means you'll need to balance two competing goals:\n",
        "\n",
        "1) The food has to be really tasty; and also, crucially:\n",
        "2) If you're going to go all the way out to Buford Highway, it better be something that you can't also get in Emory Village. Otherwise, why go to all the trouble of getting there?!\n",
        "\n",
        "Or, to give an example involving actual food: you don't want to go all the way out to Buford Highway to get pizza. Even if the pizza on Buford Highway is pretty tasty, you can get pizza anywhere in town. How can you find out what is distintively tasty on Buford Highway?  \n",
        "\n",
        "If you looked up the Yelp reviews for the all restaurants on Buford highway and sorted by score, you would get an answer to the question of what's the tastiest. But it still won't help solve the problem of what's *distintively tasty* on Buford Highway--like hot pot, for example, which is something that you can't get in Emory Village.   \n",
        "\n",
        "So you need a way to tell the difference between what's tasty and what's distinctively tasty. To do so, you need to distinguish between four categories of food. Food that, on Buford Highway, is:\n",
        "\n",
        "- both tasty and distinctive (e.g. hot pot)\n",
        "- tasty but not distinctive (e.g. pizza)\n",
        "- distinctive but not tasty (e.g. tacos-- tho I'm open to disagreement here)\n",
        "- neither tasty nor distinctive (e.g. Taco Bell--again, open to disagreement).\n",
        "\n",
        "These categories are what tf-idf helps you measure. Term frequencies can be assessed according to the same criteria. A term might be:\n",
        "\n",
        "- Frequently used in the corpus, and used especially frequently in one particular document <-- Interesting!\n",
        "- Frequently used in the corpus, but used frequently in equal measure across all documents <-- Less interesting\n",
        "- Infrequently used in the corpus, but nonetheless used frequently in one particular document <-- Potentially interesting\n",
        "- Infrequently used in in the corpus and also infrequently used in the corpus consitently across all documents <-- Not interesting\n",
        "\n",
        "It's the words that are especially frequent in one document that are most interesting to us, and the ones that tf-idf helps us identify. To see how, let's take a look at our next dataset--a bigger one--Yelp reviews from restaurants in three cities: Atlanta, New York, and San Francisco."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovRWSCyjFGPL"
      },
      "source": [
        "## Oveview of Yelp Review Data ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAkuQmhJFGPM"
      },
      "source": [
        "In this lesson, we're going to use tf-idf to study Yelp reviews for restaurants in Atlanta. This dataset was created by [Naitian Zhou](https://naitian.org/) (who we'll zoom with later in the semester) for a project that we've been working on, along with Lucy Li, involving questions of taste and authenticity in US restaurant reviews, inspired by work by [Sara Kay](https://ny.eater.com/2019/1/18/18183973/authenticity-yelp-reviews-white-supremacy-trap), [Yiwei Luo, Kristina Gligoric, and Dan Jurafsky](https://arxiv.org/pdf/2307.07645) (which you read for today), and [Sharon Zukin, Scarlett Lindeman, and Laurie Hurson](https://journals-sagepub-com.proxy.library.emory.edu/doi/full/10.1177/1469540515611203). All found instances of racial and ethnic bias in such reviews, and over the next few class sessions, we're going to see what we can find out that might confirm or refute these findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g9NfJ3zFGPM"
      },
      "source": [
        "## Pre-processing: prepare the reviews\n",
        "\n",
        "Tf-idf works on sets of documents--individual reviews in our case. We'll be using another (new to us) library, scikit-learn, in order to count the words in the reviews. But before we do, we'll need to get the reviews out of a .jsonl file and into a list, with each review stored as its own string.\n",
        "\n",
        "The reviews are stored in a .jsonl file that is zipped and stored on my Google Drive. Below is some code to get the zipped jsonl file from Google Drive, unzip it, and format the review text into a list for processing.\n",
        "\n",
        "First, download the file:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading large files from Google Drive\n",
        "# https://github.com/wkentaro/gdown\n",
        "import gdown\n",
        "\n",
        "# then download the zip files\n",
        "# atlanta\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1gIm9NcoeY1gn9EQjRr2MojGRJ-fpBSqz', quiet=False)\n",
        "\n",
        "# we'll use these next class, maybe\n",
        "# san francisco\n",
        "# gdown.download('https://drive.google.com/uc?export=download&id=1NU19CyDbRVDJfGwV9kpV-JDn5OLPuNlp', quiet=False)\n",
        "\n",
        "# new york\n",
        "# gdown.download('https://drive.google.com/uc?export=download&id=1tzL-k2wMqskcDUiA5a_VD7Z3ezu6GNJ3', quiet=False)"
      ],
      "metadata": {
        "id": "6g---LWfQavS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, unzip it:"
      ],
      "metadata": {
        "id": "QbGjN5sc-Sdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip it\n",
        "!unzip Atlanta-random.jsonl.zip\n"
      ],
      "metadata": {
        "id": "Y1wGcpxVReXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, process the data. So we can take a quick look at everything that's in the json file, we'll pull it into a dataframe first.\n",
        "\n",
        "Note that while this code is written to process this particular dataset, you'll usually need to write some sort of file/text pre-processing code in order to use any particular library/method/tool. You'll get very familiar with writing code like this by the end of the course!"
      ],
      "metadata": {
        "id": "PTV2ITIs-WIL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "KpRMaDrMFGPM"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os             # for directory/file manipulation\n",
        "import json           # for json\n",
        "import pandas as pd   # for dataframes\n",
        "\n",
        "# read in the file\n",
        "atlanta_reviews_df = pd.read_json(path_or_buf=\"./Atlanta-random.jsonl\", lines=True)\n",
        "\n",
        "len(atlanta_reviews_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take a quick look at the top\n",
        "atlanta_reviews_df.head()"
      ],
      "metadata": {
        "id": "AMR-5HYH2gYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! But what we really want is what's in the \"comment\" column, and in particular we want the value of the \"text\" key. So let's make a list with only that."
      ],
      "metadata": {
        "id": "YsX1j01itbgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first extract the 'comment' values from the dataframe\n",
        "comments = atlanta_reviews_df['comment'].tolist()\n",
        "\n",
        "# create list to store reviews\n",
        "reviews = []\n",
        "\n",
        "# iterate through the comments and append the reviews to the list\n",
        "for comment in comments:\n",
        "  reviews.append(comment['text'])\n",
        "\n",
        "# print out the first one to check\n",
        "reviews[0]"
      ],
      "metadata": {
        "id": "7W_Xu9XSNh56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oops! There's still some HTML in there. Let's do a quick cleaning pass."
      ],
      "metadata": {
        "id": "c8F5KIp1vBRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# new array w/ clean text\n",
        "reviews_clean = []\n",
        "\n",
        "for review in reviews:\n",
        "    soup = BeautifulSoup(review, \"html.parser\")\n",
        "    text = soup.get_text(separator=' ')\n",
        "\n",
        "    reviews_clean.append(text)"
      ],
      "metadata": {
        "id": "8Kyibj8MvA9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One last thing. Let's make a set of IDs so we can get a sense of what restaurant each review is about."
      ],
      "metadata": {
        "id": "RUign3uwLQsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the 'business' values from the dataframe\n",
        "businesses = atlanta_reviews_df['business'].tolist()\n",
        "\n",
        "# create list to store business aliases\n",
        "aliases = []\n",
        "\n",
        "# iterate through the business and append the alias to the list\n",
        "for business in businesses:\n",
        "  aliases.append(business['alias'])\n",
        "\n",
        "# extract ratings\n",
        "ratings = atlanta_reviews_df['rating'].tolist()\n",
        "\n",
        "# create list to store IDs\n",
        "ids = []\n",
        "\n",
        "# now put them all together into IDs\n",
        "for i, alias in enumerate(aliases):\n",
        "  id = alias + \"-review\" + str(i) + \"-\" + str(ratings[i]) + \"stars\"\n",
        "  ids.append(id)\n",
        "\n",
        "# print out the first one to check\n",
        "ids[0]"
      ],
      "metadata": {
        "id": "YJr681JeLVgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK. Looks like we're finally ready to go!"
      ],
      "metadata": {
        "id": "JzjmxRoFvumL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXRqmpZbFGPO"
      },
      "source": [
        "## To the TF-IDF calculation... or not (yet)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl0Q4UiEFGPO"
      },
      "source": [
        "Like many other methods, we can calculate tf-idf with just a few lines of code. [Here is another notebook](https://colab.research.google.com/drive/1dptJ366O9TG9a77g-qX8f4Glf_2A8LVd?usp=sharing) that presents everything below in a much more streamlined fashion.\n",
        "\n",
        "But because the TF-IDF calculation relies on a process that is, in itself, a pre-processing step for many future methods, and is in itself a little heady, we're going to spend some time today loooking under the hood.\n",
        "\n",
        "So without further ado, introduing.... the `CountVectorizer`!\n",
        "\n",
        "## But wait... the count what?\n",
        "\n",
        "When humans make sense of language, they interpret sequences of words through rules of grammar and syntax. Computers don't need to do that (although we will learn some ways to do that later in the course). To model langauge computationally, it's far easier to turn words with numbers, and then apply statistical measures and modeling approaches to the numbers that represent the words.\n",
        "\n",
        "TF-IDF, as well as machine learning approaches including language modeling, topic modeling, similarity measures, classification, and clustering--essentially, the set of methods we'll be learning in the next few units of this course--all rely on this basic transformation.\n",
        "\n",
        "## Introducing scikit-learn!\n",
        "\n",
        "We've now reached another milestone--our first use of scikit-learn (often abbreviated as sk-learn), Python's major machine learning library, which also happens to be crucial to many of the more advanced methods named above.\n",
        "\n",
        "We're going to use this to transform our words into numbers.\n",
        "\n",
        "We'll begin by importing sk-learn's `CountVectorizer`, which [converts a collection of text documents into a matrix of token counts](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
        "\n",
        "I think I've used the word \"token\" in passing before, but here I'll take a minute to formally define it, along with some related terms.\n",
        "\n",
        "## Tokens, features, document-term matrices\n",
        "\n",
        "According to the [Stanford NLP group](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html), a *token* is \"an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\" The unit of the token is usually the word, but it can also be the sentence, the subword, or anything else that makes sense for that particular task.\n",
        "\n",
        "Take this famous phrase, for example:\n",
        "\n",
        "\"To be or not to be\"\n",
        "\n",
        "This line has six tokens: \"to\", \"be\", \"or\", \"not\", \"to\", \"be\".\n",
        "\n",
        "It has four features: \"to\", \"be\", \"or\", \"not\"\n",
        "\n",
        "But wait, what is a feature?\n",
        "\n",
        "In this case, a *feature* is a unique token in the corpus. (Caveat: features, like tokens, can actually be anything that makes sense for the task, but for the purposes of turning words into numbers, features are most often unique words, or \"terms,\" as they're also sometimes called).\n",
        "\n",
        "**When sk-learn's CountVectorizer does its thing, it first *tokenizes* all of the documents in the corpus--that is, it breaks up each document into its individual tokens--and then then creates a *document-term matrix* that counts up how many times each term, or feature, appears in each document.**\n",
        "\n",
        "For example, the document-term matrix for the line above might look something like this:\n",
        "\n",
        "|   | to | be | or | not |\n",
        "|---|----|----|----|-----|\n",
        "|   | 2  | 2  | 1  | 1   |\n",
        "\n",
        "\n",
        "If we add in the second part of that phrase as a new document, we might get something like this:\n",
        "\n",
        "\n",
        "|         | to | be | or | not | that | is | the | question |\n",
        "|---------|----|----|----|-----|------|----|-----|----------|\n",
        "| line 1  | 2  | 2  | 1  | 1   | 0    | 0  | 0   | 0        |\n",
        "| line 2  | 0  | 0  | 0  | 0   | 1    | 1  | 0   | 0        |\n",
        "\n",
        "\n",
        "But enough of vectorizing by hand; let's try it out using sk-learn!\n",
        "\n",
        "\n",
        "## Importing sk-learn's CountVectorizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import CountVectorizer from sk-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "I6Pr_3h_XVXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize a teeny dataset\n",
        "\n",
        "Now let's vectorize a teeny dataset we can see:"
      ],
      "metadata": {
        "id": "tA8FDpIUXY-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here's our dataset: the chorus from Taylor Swift's \"Tortured Poets Department\"\n",
        "# in which each line is its own document\n",
        "dataset = [\n",
        "    'And who\\'s gonna hold you like me?',\n",
        "    'And who\\'s gonna know you, if not me?',\n",
        "    'I laughed in your face and said',\n",
        "    '\\\"You\\'re not Dylan Thomas, I\\'m not Patti Smith',\n",
        "    'This ain\\'t the Chelsea Hotel, we\\'rе modern idiots\\\"',\n",
        "    'And who\\'s gonna hold you like me?',\n",
        "]"
      ],
      "metadata": {
        "id": "aUmD2j_gz_Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quickly strip out the single quote marks from each element of the dataset above\n",
        "for item in dataset:\n",
        "  item.replace(\"'\", \"\")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "ftVmhM2Ozu0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now instantiate the CountVectorizer object\n",
        "# note that this is the same conceptual process we used to instantiate\n",
        "# the VADER sentiment analysis object, and the spaCy document object\n",
        "cv=CountVectorizer()\n",
        "\n",
        "# this steps generates document-term matrix for the doc;\n",
        "# it's required before you do almost anything else\n",
        "dtm=cv.fit_transform(dataset)\n",
        "\n",
        "# this method gives us the feature names that the CountVectorizer vectorized:\n",
        "features = cv.get_feature_names_out()\n",
        "\n",
        "# this method turns our doc-term matrix into an array that can be manipulated:\n",
        "dtm_array = dtm.toarray()\n",
        "\n",
        "print(\"All of the features in our dataset:\")\n",
        "print(str(features))\n",
        "\n",
        "print (\"\\nAnd their counts in each of the \\\"documents,\\\" each of which is really\")\n",
        "print(\"just a single line of the song:\")\n",
        "print(dtm_array)"
      ],
      "metadata": {
        "id": "0xYmVnryXVQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here is some code that uses dataframes to make the above slightly more legible\n",
        "\n",
        "df = pd.DataFrame(data=dtm_array,columns=features)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "CIJqsZ-UXVNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a minute to figure out what we're looking at:\n",
        "\n",
        "* Each column is a feature, or \"term,\" labeled with the name of the term, which in this case is a unique token\n",
        "* Each row is a line, labeled in order of being ingested\n",
        "* The \"1\" in row 0 of the \"and\" column means that the term \"and\" appears 1 time in the first line... and so on.  \n",
        "\n",
        "## Vectorizing a dataset from a set of files\n",
        "\n",
        "The reality is that you almost always will be vectorizing a dataset from a set of files, and not a list that you type in by hand. This is how you'd do it with the Yelp reviews dataset that we loaded earlier:"
      ],
      "metadata": {
        "id": "8hKSlbMyX1qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the vectorizer, as before\n",
        "cv=CountVectorizer()\n",
        "\n",
        "# generates document-term matrix for all the docs\n",
        "dtm=cv.fit_transform(reviews_clean)\n",
        "\n",
        "# get the feature names aka terms\n",
        "features = cv.get_feature_names_out()\n",
        "\n",
        "# take a look at some features in the middle\n",
        "print(features[3000:3099])"
      ],
      "metadata": {
        "id": "0c_514MXXVLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can also check the overall shape of the doc-term matrix\n",
        "dtm.shape"
      ],
      "metadata": {
        "id": "fQ7TRvutXVFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does this tell you about how many documents there are? What about the number of features?**"
      ],
      "metadata": {
        "id": "-bCKz62UYqx9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQhi3yU5Y1eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLFz7vmcY1aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0UgMJTQY1Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRnrqr-hXU3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helpful CountVectorizer Parameters\n",
        "\n",
        "Since we're on the subject of the `CountVectorizer`, here are a few more helpful `CountVectorizer` parameters to know about:\n",
        "\n"
      ],
      "metadata": {
        "id": "NOCSfPW0ZBRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lowercase all words -- this is True by default, but if you want to preserve case,\n",
        "# you can set lowercase to False\n",
        "cv_caps = CountVectorizer(lowercase=False)\n",
        "\n",
        "# generates document-term matrix for all the docs\n",
        "dtm2=cv_caps.fit_transform(reviews_clean)\n",
        "\n",
        "# check the shape\n",
        "dtm2.shape"
      ],
      "metadata": {
        "id": "GFeW75S2XU2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, there are more terms since it's not merging the uppercase and the lowercase versions of each word together.\n",
        "\n",
        "Another parameter to know about has to do with stopwords. These are common words like \"and\", \"not\", \"or\", etc. that are not usually that interesting."
      ],
      "metadata": {
        "id": "rvBkKmTfZTbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the built-in English stopwords list\n",
        "cv_no_stops = CountVectorizer(stop_words='english')\n",
        "\n",
        "# generates document-term matrix for all the docs\n",
        "dtm3=cv_no_stops.fit_transform(reviews_clean)\n",
        "\n",
        "# check the shape\n",
        "dtm3.shape"
      ],
      "metadata": {
        "id": "Q7xjshObXUz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use a custom stopwrods list\n",
        "cv_no_stops = CountVectorizer(stop_words=['food','review','atlanta'])\n",
        "\n",
        "# generates document-term matrix for all the docs\n",
        "dtm4=cv_no_stops.fit_transform(reviews_clean)\n",
        "\n",
        "# check the shape\n",
        "dtm4.shape"
      ],
      "metadata": {
        "id": "h0v9cqrUXUvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One last helpful feature of CountVectorizer is that you can tell it very easily to tokenize by ngrams as well as words. To wit:"
      ],
      "metadata": {
        "id": "PQapLcgOZn16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_cv = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
        "\n",
        "# generates document-term matrix for all the docs\n",
        "dtm5=bigram_cv.fit_transform(reviews_clean)\n",
        "\n",
        "# get the feature names -- bigrams in this case\n",
        "features = bigram_cv.get_feature_names_out()\n",
        "\n",
        "# take a look at some features in the middle\n",
        "print(features[3000:3099])"
      ],
      "metadata": {
        "id": "EbS-iC8DXUmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Return to TF-IDF\n",
        "\n",
        "At long last, we're ready to calculate the TF-IDF scores for our corpus!\n",
        "\n",
        "We're going to do this step by step at first, to make sure that you understand each of the processes, and then at the end, you'll see how to do this in only a few lines of code."
      ],
      "metadata": {
        "id": "zfne1yT-af5I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dFLnI8wGFGPO"
      },
      "outputs": [],
      "source": [
        "# import the TF-IDF libraries\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCVn3KaCFGPO"
      },
      "source": [
        "## Create document-term matrix\n",
        "\n",
        "We'll use a doc-term matrix to calculate tf-idf. We already know how to do this step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aL3iCZvyFGPP"
      },
      "outputs": [],
      "source": [
        "#instantiate CountVectorizer()\n",
        "cv=CountVectorizer(stop_words='english') # using stopwords this time\n",
        "\n",
        "# this steps generates document-term matrix for the docs\n",
        "dtm=cv.fit_transform(reviews_clean)\n",
        "\n",
        "# check shape\n",
        "dtm.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DqZqMk7FGPP"
      },
      "source": [
        "**How many reviews do we have in this document-term matrix and how many unique features/terms?**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hWsGNBXePp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bAHn_YzRePiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVdLPubmePZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2bmmtdUFGPP"
      },
      "source": [
        "That line tells us that we have 34370 rows, one for each review in the dataset, and 35151 columns, one for each word (minus single character words, which the tokenizer excludes, as well as the default stopwords, which we've indicated should be excluded with the stop_words='english' parameter above).\n",
        "\n",
        "We can also look at the whole vocabulary like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7jqSKjVFGPP"
      },
      "outputs": [],
      "source": [
        "cv.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm20uYAMFGPQ"
      },
      "source": [
        "The numbers above are the indices for each feature, not the word counts. But we can use the indicies in order to generate our word counts.\n",
        "\n",
        "Note that we did a similar thing above using `cv.get_feature_names_out()`. This is arguably more efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C0lGcAUFGPQ"
      },
      "outputs": [],
      "source": [
        "sum_words = dtm.sum(axis=0) # sum_words is a vector that contains the number of times each word appears in all\n",
        "                            # the docs in the corpus. In other words, we are summing the elements for each column\n",
        "                            # of the doc-term matrix and storing those counts as a vector\n",
        "\n",
        "# then sort the list of tuples that contain the word and their occurrence in the corpus.\n",
        "# tuples are Python's name for single variables that actually store multiple variables,\n",
        "# like the word and index in the vocabulary attribute above\n",
        "\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()] # rememeber list comprehension!\n",
        "\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "\n",
        "# display the top 20\n",
        "words_freq[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-8PinBOFGPQ"
      },
      "source": [
        "We can already see some words with the most counts don't seem too distinctive: \"food\" and \"place,\" for example. It's not surprising that those are the most frequently occurring words since these are reviews about restaurants.\n",
        "\n",
        "So now let's calculate the IDF values so that we can balance them out. While we could also calculate these by hand, sk-learn makes it really easy to do it in a few lines of code, so we'll use that instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCOR9fSPFGPQ"
      },
      "source": [
        "## Initialize TfidfTransformer\n",
        "\n",
        "When you initialize TfidfTransformer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf. The recommended way to run `TfidfTransformer` is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in document length, and, overall, they'll produce more meaningful tf–idf scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TJPPDspJFGPQ"
      },
      "outputs": [],
      "source": [
        "# Call tfidf_transformer.fit on the word count vector we computed earlier.\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(dtm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUYrCHl2FGPR"
      },
      "source": [
        "## Print inverse document frequence (idf) values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "EMZTkxNIFGPR"
      },
      "outputs": [],
      "source": [
        "# make a dataframe for the idf values\n",
        "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_weights\"])\n",
        "\n",
        "# sort ascending\n",
        "df_idf.sort_values(by=['idf_weights'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG5DaM5nFGPR"
      },
      "source": [
        "In the table above, the words at the top are those that appear in the most number of reviews, across all of the dataset; and the words at the bottom are those that appear in the least number of reviews.\n",
        "\n",
        "Once again, it makes sense that words like \"food\" and \"good\" are at the top.\n",
        "\n",
        "The words at the bottom appear only once across the entire dataset.\n",
        "\n",
        "## IDF by the numbers\n",
        "\n",
        "But what are these numbers that we're looking at?\n",
        "\n",
        "The most direct formula would be **N/df<sub>i</sub>**, where N represents the total number of reviews (or documents) in the dataset, and df is the number of documents in which the term appears.\n",
        "\n",
        "However, many implementations of tf-idf, including scikit-learn, which we are using, normalize the results with additional operations.\n",
        "\n",
        "In tf-idf, normalization is generally used in two ways, and for two reasons: first, to prevent bias in term frequency from terms in shorter or longer documents; and second, as above, to calculate each term’s idf value.\n",
        "\n",
        "Scikit-learn’s implementation of tf-idf represents N as **N+1**, calculates the natural logarithm of **(N+1)/df<sub>i</sub>**, and then adds **1** to the final result. Here is this same thing formatted slightly more nicely:\n",
        "\n",
        "<img src=\"http://lklein.com/wp-content/uploads/2019/10/Screen-Shot-2019-10-02-at-11.52.31-PM.png\">\n",
        "\n",
        "**Important note!** This is only one way to calculate TF-IDF. There are many, many versions. The number itself isn't important. It's the *ranking* that the number enables that's most interesting to us. Because one you have the IDF values, you can now compute the tf-idf scores for any document or set of documents.\n",
        "\n",
        "So now let’s compute tf-idf scores for the documents in our corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKBfk1n5FGPR"
      },
      "source": [
        "## Produce & print tf-idf scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlWSpEh1FGPR"
      },
      "source": [
        "Once you have the idf values, you can compute the tf-idf scores for any document or set of documents. Let’s compute tf-idf scores for the documents in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1up_4waUFGPS"
      },
      "outputs": [],
      "source": [
        "# tf-idf scores\n",
        "tf_idf_vector=tfidf_transformer.transform(dtm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHntwARwFGPS"
      },
      "source": [
        "Now, let’s print the tf-idf values of the first document to see if they make sense.\n",
        "\n",
        "We'll place the tf-idf scores from the first document into a pandas dataframe and sort the dataframe in descending order of scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "y13fcHeoFGPS"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "feature_names = cv.get_feature_names_out()\n",
        "\n",
        "#get tfidf vector for first document\n",
        "first_document_vector=tf_idf_vector[0]\n",
        "\n",
        "# print the review text for the first doc\n",
        "print(textwrap.fill(reviews_clean[0],100))\n",
        "\n",
        "#print the scores for the first doc\n",
        "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
        "df.sort_values(by=[\"tfidf\"],ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPpZCZi9FGPS"
      },
      "source": [
        "Notice that only certain words have scores. This is because only the words in this document have a tf-idf score and everything else, from other documents, shows up as zeroes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try another:\n"
      ],
      "metadata": {
        "id": "0PZXW0N_-kLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_num = 3000\n",
        "\n",
        "#get tfidf vector for another document\n",
        "first_document_vector=tf_idf_vector[review_num]\n",
        "\n",
        "# print the review text for the first doc\n",
        "print(textwrap.fill(reviews_clean[review_num],100))\n",
        "\n",
        "#print the scores for the first doc\n",
        "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
        "df.sort_values(by=[\"tfidf\"],ascending=False)"
      ],
      "metadata": {
        "id": "2rDUuOdF-pgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sOGlaANFGPS"
      },
      "source": [
        "## tf-idf: the fast way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JtgLxLLFGPS"
      },
      "source": [
        "Since we're now tf-idf pros, we're going to use scikit-learn's all-in-one tf-idf vectorizer to do this entire notebook again in two lines of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jLkZlv8JFGPS"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer=TfidfVectorizer(stop_words='english', use_idf=True) # excludings stopwords again\n",
        "\n",
        "# send in all your docs here\n",
        "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(reviews_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "it0IQaHHFGPT"
      },
      "outputs": [],
      "source": [
        "# place tf-idf values for all docs in a pandas dataframe\n",
        "tfidf_df = pd.DataFrame(tfidf_vectorizer_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "tfidf_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "I-zrPKE4FGPT"
      },
      "outputs": [],
      "source": [
        "# Add row for number of times word appears in all documents\n",
        "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**And we're done! 🎉 🎉 🎉**\n",
        "\n",
        "Now let's explore the results..."
      ],
      "metadata": {
        "id": "SgUSerfDM1-n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJdO-zcmFGPT"
      },
      "source": [
        "## Let's explore!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5IAAzoUFGPT"
      },
      "source": [
        "We can look at specific words and how they appear in our reviews dataset. I've entered five words below that we might want to investigate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zDoe_YUZFGPU"
      },
      "outputs": [],
      "source": [
        "tfidf_slice = tfidf_df[['wings', 'dumplings', 'tacos', 'pancakes', 'bao']]\n",
        "tfidf_slice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T15vnZn3FGPU"
      },
      "source": [
        "**Does this output make sense? What does it tell you about which articles you might want to go read? What about some research questions you might ask of this corpus using TF-IDF?**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching for multiple terms\n",
        "\n",
        "Not too much different than above, but you don't need to include just one term as part of your slice."
      ],
      "metadata": {
        "id": "0ca1n0cwF3yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_slice = tfidf_df[['chicken', 'waffles']]\n",
        "\n",
        "# filter out zero values\n",
        "tfidf_slice = tfidf_slice[\n",
        "    ~((tfidf_slice['chicken'] == 0) | (tfidf_slice['waffles'] == 0))\n",
        "]\n",
        "\n",
        "# now calculate the sum of 'chicken' and 'waffles' columns\n",
        "tfidf_slice['total'] = tfidf_slice['chicken'] + tfidf_slice['waffles']\n",
        "\n",
        "# sort by total\n",
        "slice_sorted = tfidf_slice.sort_values(by=['total'],ascending=False)\n",
        "\n",
        "print (slice_sorted[:10])"
      ],
      "metadata": {
        "id": "RT-bqmDJRxkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This suggests that we should check out the reviews with these indices if we want to read about chicken and waffles. Let's try it!"
      ],
      "metadata": {
        "id": "aZCch2VcTEn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = slice_sorted[:10].index.tolist()\n",
        "\n",
        "for index in indexes:\n",
        "  print(str(ids[index]))\n",
        "  print(textwrap.fill(reviews_clean[index],100) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "b8tGqZR1Tqyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying the top terms for any particular document\n",
        "\n",
        "The second major use of TF-IDF is to characterize the most significant words in any particular document. Here's some code that will do that for the first document in the corpus:"
      ],
      "metadata": {
        "id": "svrJxNBCHfDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pull out the row at location 0\n",
        "# replace the index number to pull up another doc\n",
        "idx = 0\n",
        "\n",
        "print(\"ID: \" + str(ids[idx]))\n",
        "doc_row = tfidf_df.iloc[idx]\n",
        "print(\"Top words: \")\n",
        "\n",
        "# sort by td-idf scores top to bottom\n",
        "doc_row = doc_row.sort_values(ascending=False)\n",
        "\n",
        "# print out the top ten\n",
        "print (doc_row[:10])"
      ],
      "metadata": {
        "id": "x1pgLHXZHh6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Most unique words in a corpus\n",
        "\n",
        "Oh and of course! Here are the most unique words in the corpus overall!\n",
        "\n",
        "Recall from the previous lesson that this is distinct from the most *frequent* words in the corpus."
      ],
      "metadata": {
        "id": "oxw966sqHn2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add in a row with the total TF-IDF scores\n",
        "tfidf_df.loc['Total_TFIDF'] = tfidf_df.sum()\n",
        "\n",
        "# sort by Total_TFIDF values, high to low\n",
        "tfidf_df.sort_values(by=['Total_TFIDF'], axis=1, ascending=False, inplace=True)\n",
        "\n",
        "tfidf_df"
      ],
      "metadata": {
        "id": "SDzFG0ETHplQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the same thing formatted slightly more nicely.\n",
        "\n",
        "Note use of the Python `range` method to generate an iterator."
      ],
      "metadata": {
        "id": "fhduMsH3Huap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_terms = list(tfidf_df.columns.values.tolist())\n",
        "\n",
        "for i in range(25):\n",
        "  print(str(i) + \". \" + str(sorted_terms[i]))\n"
      ],
      "metadata": {
        "id": "GFzIISEyHtvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Searching/sorting by tf-idf score\n",
        "\n",
        "One question you often want to ask about tf-idf scores relates to individual words-- more specifically, which documents have the highest tf-idf scores for a specific word.  \n",
        "\n",
        "You might want to search/sort this way if you were curious, for example, which documents were most uniquely about, say, food:"
      ],
      "metadata": {
        "id": "n0YOmV9mNoYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new dataframe for id lookup\n",
        "tfidf_df = pd.DataFrame(tfidf_vectorizer_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# add in a column for the titles of each article for future reference\n",
        "tfidf_df['IDs'] = ids"
      ],
      "metadata": {
        "id": "LKkK4RycNtvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_slice_sorted = tfidf_df[['IDs', 'boba']].sort_values(by=['boba'], ascending=False)\n",
        "\n",
        "# print out the top ten\n",
        "print (tfidf_slice_sorted[:10])"
      ],
      "metadata": {
        "id": "GApQ5jiqOmoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above list then suggests the reviews that you should prioritize reading if your interest was in boba."
      ],
      "metadata": {
        "id": "YLDc3ptuOzKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching for multiple terms\n",
        "\n",
        "Not too much different than the above"
      ],
      "metadata": {
        "id": "w6H-9awTPRD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_slice_sorted = tfidf_df[['IDs', 'boba', 'coffee']].sort_values(by=['boba', 'coffee'], ascending=True)\n",
        "\n",
        "# print out the top ten\n",
        "print (tfidf_slice_sorted[:10])\n"
      ],
      "metadata": {
        "id": "Q9xpMlrlPWlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsYyPHNqFGPU"
      },
      "source": [
        "## Cosine similarity\n",
        "\n",
        "Just so you know how to do everything in that Pudding Hip-Hop feature, this is how you calculate cosine similiarty between documents on the basis of their tf/idf scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUP3kE_wFGPU"
      },
      "outputs": [],
      "source": [
        "# CALCULATE SIMILARITY TO FIRST DOC\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(tfidf_vectorizer_vectors[0:1], tfidf_vectorizer_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lauren F. Klein wrote version 1.0 of this notebook in 2019 based of tutorials by [Matthew Lavin](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf) and [Kavita Ganesan](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XZVlcOdKhSw). Dan Sinykin supplemented it with material from Melanie Walsh's chapter [TF-IDF](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/Text-Analysis/TF-IDF.html) in 2020. Lauren Klein updated it again in 2021, 2022, and 2024.*\n",
        "\n"
      ],
      "metadata": {
        "id": "zwvKWaReNX2h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j60BQngDFGPU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bG5DaM5nFGPR"
      ],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}