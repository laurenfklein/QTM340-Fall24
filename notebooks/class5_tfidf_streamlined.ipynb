{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDtvbdozFGPC"
      },
      "source": [
        "## TF-IDF, streamlined\n",
        "\n",
        "Last class, we went under the hood of TF-IDF in order to understand how it's calculated, and how sk-learn's `CountVectorizer` plays a part. We might have learned a lot, but how to actually calculate TF-IDF was buried at the bottom. Also, I didn't show you how to associate specific words in specific articles with their TF-IDF scores, which is how the measure is most commonly used.\n",
        "\n",
        "Hence this notebook: TF-IDF, streamlined.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovRWSCyjFGPL"
      },
      "source": [
        "## The Emory Wheel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAkuQmhJFGPM"
      },
      "source": [
        "As in the previous lesson, our corpus will be the articles published by *The Emory Wheel* betweeen 2014 and 2019.\n",
        "\n",
        "This dataset was created by Honggang Min and Kexin Guan for their final project in the 2019 iteration of this course, and was generously transfered back to me for future class use.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g9NfJ3zFGPM"
      },
      "source": [
        "## Pre-processing #1: Downloading the documents\n",
        "\n",
        "Tf-idf works on sets of documents. In this particular case, the documents are individual .txt files that are stored in a zip file on my Google Drive. Below is some code to get the data from Google Drive, unzipped, and formatted into a list for processing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading large files from Google Drive\n",
        "# https://github.com/wkentaro/gdown\n",
        "import gdown\n",
        "\n",
        "# then download the zip file\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1SUWUVswaY_RDLhzFruQIDJe-i6I3gznC', quiet=False)"
      ],
      "metadata": {
        "id": "6g---LWfQavS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip it\n",
        "!unzip wheel-clean.zip"
      ],
      "metadata": {
        "id": "Y1wGcpxVReXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing #2: From individual files to a single list\n",
        "\n",
        "In order to calculate our tf-idf scores, we'll need to get all of the documents into a single Python list, with each document stored as a single (string) item in the list.\n",
        "\n",
        "Note that this is custom code for processing this particular set of documents. While the specific code will (almost) always be different depending on the particular storage location and format of the files, you will always need *some* pre-processing code like this in order to get the files into the format that any particular method requires.  "
      ],
      "metadata": {
        "id": "g5eo_JybfVQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "KpRMaDrMFGPM"
      },
      "outputs": [],
      "source": [
        "# import this library for directory/file manipulation\n",
        "import os\n",
        "\n",
        "# set the base directory -- note that this may need to change if you've saved a copy\n",
        "# of this notebook elsewhere\n",
        "base_dir = \"wheel-clean/\"\n",
        "\n",
        "# read in a list of all the filenames\n",
        "docs = os.listdir(base_dir)\n",
        "\n",
        "# a list for storing the text of all the docs\n",
        "wheel_docs = []\n",
        "\n",
        "# a list for storing the titles of the docs -- not necessary, just makes recall easier\n",
        "text_titles = []\n",
        "\n",
        "# iterate through each of the docs in the directory\n",
        "for doc in docs:\n",
        "    with open(base_dir + doc, \"r\") as file:     # open the doc file\n",
        "        text = file.read()                      # read the contents of the file\n",
        "        wheel_docs.append(text)                 # append the contents of the file to our all_docs list for future manipulation\n",
        "        text_titles.append(str(doc))            # append the title of the doc to another list for future reference\n",
        "\n",
        "# just take a look at the first item to be sure it worked\n",
        "print(\"Filename: \" + str(text_titles[0]) + \"\\n\")\n",
        "print(wheel_docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXRqmpZbFGPO"
      },
      "source": [
        "## To the TF-IDF calculation!\n",
        "\n",
        "Here are the few lines of code that make use of scikit-learn's all-in-one tf-idf vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import our required library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# to exclude stopwords, add the argument `stop_words='english'`\n",
        "tfidf_vectorizer=TfidfVectorizer(stop_words='english', use_idf=True)\n",
        "\n",
        "# send in all your docs here\n",
        "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(wheel_docs)\n"
      ],
      "metadata": {
        "id": "I6Pr_3h_XVXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**And we're done! ðŸŽ‰ ðŸŽ‰ ðŸŽ‰**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Now, to explore the results..."
      ],
      "metadata": {
        "id": "4g8pfrWAgVlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas for Python dataframes\n",
        "import pandas as pd\n",
        "\n",
        "# send the output of the vectorizer into a dataframe\n",
        "# tfidf_df = pd.DataFrame(tfidf_vectorizer_vectors.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "tfidf_df = pd.DataFrame(tfidf_vectorizer_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# add in a column for the titles of each article for future reference\n",
        "tfidf_df['Title'] = text_titles\n",
        "\n",
        "# print out our dataframe of tf-idf scores\n",
        "# note that this is not super readable yet, but the next few cells will help us make sense of the results\n",
        "tfidf_df\n"
      ],
      "metadata": {
        "id": "ZJRjDJ5sgVQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Searching/sorting by tf-idf score\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "\n",
        "One question you often want to ask about tf-idf scores relates to individual words-- more specifically, which documents have the highest tf-idf scores for a specific word.  \n",
        "\n",
        "You might want to search/sort this way if you were curious, for example, which documents were most uniquely about, say, food:"
      ],
      "metadata": {
        "id": "7mQGgg75h3Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_slice_sorted = tfidf_df[['Title', 'food']].sort_values(by=['food'], ascending=False)\n",
        "\n",
        "# print out the top ten\n",
        "print (tfidf_slice_sorted[:10])"
      ],
      "metadata": {
        "id": "zhtjCeB0euGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above list then suggests the articles that you should prioritize reading if your interest was in food. For example, the first one about sustainability from November 21, 2014:"
      ],
      "metadata": {
        "id": "FO9EWOBDk4QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wheel_docs[3096]\n",
        "\n",
        "# note that I am using the index number that is listed above to pull up the document\n",
        "# the files are read in in a different order each time, so your index number for the\n",
        "# top article may not be the same, even if the same article\n",
        "# (\"2014-11-21-Sustainability-Security-Vital-to-Fu...\") will still have the top score\n"
      ],
      "metadata": {
        "id": "6vxUeT0ClM__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJdO-zcmFGPT"
      },
      "source": [
        "## Searching for multiple terms\n",
        "\n",
        "Not too much different than above, but you don't need to include just one term as part of your slice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zDoe_YUZFGPU"
      },
      "outputs": [],
      "source": [
        "tfidf_slice_sorted = tfidf_df[['Title', 'food', 'dinner', 'lunch', 'breakfast']].sort_values(by=['food', 'dinner', 'lunch', 'breakfast'], ascending=False)\n",
        "\n",
        "# print out the top ten\n",
        "print (tfidf_slice_sorted[:10])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T15vnZn3FGPU"
      },
      "source": [
        "## Displaying the top terms for any particular document\n",
        "\n",
        "The second major use of TF-IDF is to characterize the most significant words in any particular document. Here's some code that will do that for the first document in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pull out the row at location 0\n",
        "# replace the index number to pull up another doc\n",
        "doc_row = tfidf_df.iloc[0]\n",
        "\n",
        "# drop the title col b/c it messes up sorting\n",
        "doc_row = doc_row.drop('Title')\n",
        "\n",
        "# sort by td-idf scores top to bottom\n",
        "doc_row = doc_row.sort_values(ascending=False)\n",
        "\n",
        "# print out the top ten\n",
        "print (doc_row[:10])\n"
      ],
      "metadata": {
        "id": "6bfifeUpqKKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Most unique words in a corpus\n",
        "\n",
        "Oh and of course! Here are the most unique words in the corpus overall!\n",
        "\n",
        "Recall from the previous lesson that this is distinct from the most *frequent* words in the corpus."
      ],
      "metadata": {
        "id": "u1d6cB847538"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j60BQngDFGPU"
      },
      "outputs": [],
      "source": [
        "# drop the title col b/c it messes up sorting\n",
        "tfidf_df = tfidf_df.drop(columns=['Title'])\n",
        "\n",
        "# add in a row with the total TF-IDF scores\n",
        "tfidf_df.loc['Total_TFIDF'] = tfidf_df.sum()\n",
        "\n",
        "# sort by Total_TFIDF values, high to low\n",
        "tfidf_df.sort_values(by=['Total_TFIDF'], axis=1, ascending=False, inplace=True)\n",
        "\n",
        "tfidf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the same thing formatted slightly more nicely.\n",
        "\n",
        "Note use of the Python `range` method to generate an iterator.\n"
      ],
      "metadata": {
        "id": "EdGgd5pA-ZmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_terms = list(tfidf_df.columns.values.tolist())\n",
        "\n",
        "for i in range(25):\n",
        "  print(str(i) + \". \" + str(sorted_terms[i]))\n"
      ],
      "metadata": {
        "id": "r0zbgBAC8Xfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lauren F. Klein wrote version 1.0 of this notebook in 2022*\n",
        "\n"
      ],
      "metadata": {
        "id": "zwvKWaReNX2h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntT9Ivfj-jEH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}