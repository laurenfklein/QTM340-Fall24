{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBK3gVakZD2m"
      },
      "source": [
        "# Clustering\n",
        "\n",
        "In this course we've already learned about classification, which assumes we have a test set of data that we have already categorized. When we run models with labeled data, we call it _supervised learning_.\n",
        "\n",
        "But what happens when we don't have labeled data? Is it possible to learn anything about what the data contains?\n",
        "\n",
        "This scenario is known as *unsupervised learning*, and we will see that it is very possible to learn about the underlying structure of unlabeled observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoKzGw-eZD2n"
      },
      "source": [
        "## An example\n",
        "\n",
        "Here’s a scenario:\n",
        "\n",
        "Suppose you scrape thousands of news articles. And suppose you want to organize, or cluster, those documents by their prevalence of language about hurricanes vs elections. You could choose a group of words relate to hurricanes and make the x-axis the frequency with which those words appear in the documents. You could do the same for words related to elections on the y-axis. We might imagine the articles clustering into four groups: one for documents that are largely about a hurricane, another for documents largely about a election, a third for documents about neither, and a fourth for documents, however unlikely, about both.\n",
        "\n",
        "These clusters represent an underlying structure of the data. But it's still supervised, because we've labeled words as belonging to categories and used them to organize documents.\n",
        "\n",
        "Unsupervised learning applies the same basic idea, but in a high-dimensional space with one dimension for every word. As such, the space can’t be visualized. But the goal is the same as in two-dimensions: identify an underlying structure of the observed data, such that documents cluster, and each cluster is internally coherent.\n",
        "\n",
        "**Clustering algorithms** are capable of finding such structure automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOmgbcbMZD2o"
      },
      "source": [
        "## Enter k-means clustering\n",
        "\n",
        "Clustering algorithms assign each data point—for us, each document—to a discrete cluster. One of the best known clustering algorithms is k-means, an iterative algorithm that maintains a cluster assignment for each instance, and a central (\"mean\") location for each cluster. K-means iterates between updates to the assignments and the centers:\n",
        "\n",
        "1. each instance is placed in the cluster with the closest center;\n",
        "\n",
        "2. each center is recomputed as the average over points in the cluster.\n",
        "\n",
        "Let's watch [the start of a little video](https://www.youtube.com/watch?v=yR7k19YBqiw) to see how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLuoMCd7ZD2o"
      },
      "source": [
        "**NOTE:** An important property of k-means is that the converged solution depends on the initialization, and a better clustering can sometimes be found simply by re-running the algorithm from a different random starting point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDFm08SgZD2o"
      },
      "source": [
        "## Let's get to it! ##\n",
        "\n",
        "In this example, we're going to use k-means clustering to identify the latent structures within the synopses of the top 100 films of all time (per an IMDB list), a corpus created by Brandon Rose. See [the original post](http://www.brandonrose.org/top100) for a more detailed discussion on the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjguVD3qZD2o"
      },
      "source": [
        "Let's start with the imports. They include old friends: re, BeautifulSoup, nltk, pandas, sklearn, a veritable journey through our semester:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB4EF4n5ZD2o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sklearn import feature_extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus\n",
        "\n",
        "Now let's load in our corpus. Here, we're going to import four files that each map to each other: film titles, links to the films on imdb, wikipedia synopses of the films, and the associated genres for each.\n"
      ],
      "metadata": {
        "id": "eDahqp2hgcJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading large files from Google Drive\n",
        "import gdown\n",
        "\n",
        "# Download the title list file - a list of film titles, in order of top-ness\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=15FqA2-VKSoGdT3DT2IBiSCeIROQup2LH', quiet=False)\n",
        "\n",
        "# Download the url list - a list of URLS to each film on IMDB, in the same order as the first\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1LMbHmHlAZXWJN0KAIxbvfy5zB_6sAwio', quiet=False)\n",
        "\n",
        "# Download the synopses file - a list of synopses of each film, in the same order as both files above, with \"BREAKS HERE\" separating each entry\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1BDRU4wcG6Qd1zaQwyfQouufw2hmAEOKr', quiet=False)\n",
        "\n",
        "# Download the genres file - a list of associated genres, one film per line, in the same order as above\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1Phf-rcmQXneEiUra5z_l5Eky7x6FDITk', quiet=False)\n"
      ],
      "metadata": {
        "id": "xA7gNLycgbcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt4v3pOuZD2p"
      },
      "source": [
        "## Corpus Pre-Proccesing ##\n",
        "\n",
        "Now let's pre-process our corpus for clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTZnrh8iZD2p"
      },
      "outputs": [],
      "source": [
        "# import four files that each map to each other: titles, links, wikipedia synopses, and associated dgenres\n",
        "\n",
        "# first, the list of film titles, in order of top-ness\n",
        "titles = open('title_list.txt').read().split('\\n')\n",
        "\n",
        "# ensures that only the top 100 titles are read in\n",
        "titles = titles[:100]\n",
        "\n",
        "# a list of URLS to each film on IMDB, in the same order as the first\n",
        "links = open('link_list_imdb.txt').read().split('\\n')\n",
        "links = links[:100]\n",
        "\n",
        "# a list of synopses of each film, in the same order as both files above, with \"BREAKS HERE\" separating each entry\n",
        "synopses_wiki = open('synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
        "synopses_wiki = synopses_wiki[:100]\n",
        "\n",
        "synopses_clean_wiki = []\n",
        "\n",
        "for text in synopses_wiki:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    # strips html formatting and converts to unicode\n",
        "    synopses_clean_wiki.append(text)\n",
        "\n",
        "synopses_wiki = synopses_clean_wiki\n",
        "\n",
        "# now the list of associated genres, one film per line, in the same order as above\n",
        "genres = open('genres_list.txt').read().split('\\n')\n",
        "genres = genres[:100]\n",
        "\n",
        "print(str(len(titles)) + ' titles')\n",
        "print(str(len(links)) + ' links')\n",
        "print(str(len(synopses_wiki)) + ' synopses')\n",
        "print(str(len(genres)) + ' genres')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-UOVHlZD2p"
      },
      "source": [
        "Let's see what movies we have and their genres:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3ewuE2fZD2p"
      },
      "outputs": [],
      "source": [
        "for x in range(99):\n",
        "    print(titles[x], genres[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC-EMcX3ZD2p"
      },
      "source": [
        "Let's check out the first synopsis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RekvTdMPZD2p"
      },
      "outputs": [],
      "source": [
        "print(synopses_wiki[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUU5x-yHZD2p"
      },
      "outputs": [],
      "source": [
        "# now let's get the imdb synopses as well\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1NofscgV8eWPhgM4QxSAN3cyIhwENqIk4', quiet=False)\n",
        "\n",
        "synopses_imdb = open('synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
        "synopses_imdb = synopses_imdb[:100]\n",
        "\n",
        "synopses_clean_imdb = []\n",
        "\n",
        "for text in synopses_imdb:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    #strips html formatting and converts to unicode\n",
        "    synopses_clean_imdb.append(text)\n",
        "\n",
        "synopses_imdb = synopses_clean_imdb\n",
        "\n",
        "# and check out the first\n",
        "synopses_imdb[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS_d3i4YZD2p"
      },
      "outputs": [],
      "source": [
        "# make a list with the two sets of synopses—from imdb and wikipedia\n",
        "\n",
        "synopses = []\n",
        "\n",
        "for i in range(len(synopses_wiki)):\n",
        "    item = synopses_wiki[i] + synopses_imdb[i]\n",
        "    synopses.append(item)\n",
        "\n",
        "# see what one looks like\n",
        "print(synopses[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybYxXxDJZD2q"
      },
      "source": [
        "## Now our trusty friend tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM8VA2YEZD2q"
      },
      "source": [
        "Before we can cluster our movie synopses, we need to prepare them as vectors. We're going to prepare them as *tf-idf* vectors because they provide more effective clusters than other ways of counting words, for example raw word counts.\n",
        "\n",
        "NB: Some people like to lemmatize or stem their documents before peparing the vectors, which we also now know how to do!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import spacy\n",
        "#import en_core_web_sm\n",
        "\n",
        "# load the  model\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# lemmatize the synopses\n",
        "#lemmatized_synopses = []\n",
        "\n",
        "#for synopsis in synopses:\n",
        "#    doc = nlp(synopsis)\n",
        "#    lemmatized_synopsis = \" \".join([token.lemma_ for token in doc])\n",
        "#    lemmatized_synopses.append(lemmatized_synopsis)"
      ],
      "metadata": {
        "id": "68r2JqYx-IcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS4DsG-wZD2q"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', #note new params\n",
        "                                   use_idf=True,\n",
        "                                    ngram_range=(1,3), # use n-grams from 1 to 3 words long\n",
        "                                   min_df=.2, # filter out n-grams that appear in less than 20% of documents\n",
        "                                   max_df=0.8) # ignore n-grams that appear in more than 80% of documents\n",
        "\n",
        "# tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(lemmatized_synopses)\n",
        "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(synopses)\n",
        "\n",
        "print(tfidf_vectorizer_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbR-r2kDZD2q"
      },
      "source": [
        "**What does the shape of the vector tell us?**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cKEsccq7tAEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdPOhVZitABj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JqXvJrAws__L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZK8LsU92s_M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QTz429lks_Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qFKXMfqCs2B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umqg8Wv1ZD2q"
      },
      "outputs": [],
      "source": [
        "# get our feature names for future reference\n",
        "# terms = tfidf_vectorizer.get_feature_names()\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZSLxT7yZD2q"
      },
      "outputs": [],
      "source": [
        "# get the first vector out (for the first synopsis) just to see what it looks like\n",
        "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
        "\n",
        "# place tf-idf values in a pandas data frame\n",
        "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=terms, columns=[\"tfidf\"])\n",
        "\n",
        "df.sort_values(by=[\"tfidf\"],ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W-LABZDZD2q"
      },
      "source": [
        "This is the vector for **The Godfather**. It scores high on 'don', 'family', and 'son'. How about we take a look at another one?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the first vector out (for the first synopsis) just to see what it looks like\n",
        "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[96]\n",
        "\n",
        "# place tf-idf values in a pandas data frame\n",
        "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=terms, columns=[\"tfidf\"])\n",
        "\n",
        "df.sort_values(by=[\"tfidf\"],ascending=False).head(10)"
      ],
      "metadata": {
        "id": "KkKb4i7FzL36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Any guesses as to what this one is?**"
      ],
      "metadata": {
        "id": "pSjkSbBrzkOb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EZommz-zmE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpPpbTVJtDC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p82rj_cQtDAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ct7aU9xatC-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yEvvmr-tC8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx4oln-gZD2q"
      },
      "source": [
        "## On to the clustering!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a73pJ0tfZD2q"
      },
      "source": [
        "Using our tf-idf vectors, we can now run the k-means clustering algorithm. Remember that k-means initializes with a pre-determined number of clusters. Let's somewhat arbitrarily choose 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77j52_aYZD2q"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 5\n",
        "\n",
        "km = KMeans(n_clusters=num_clusters, n_init=10) # default is also 10, but good to know\n",
        "\n",
        "clusters = km.fit_predict(tfidf_vectorizer_vectors)\n",
        "\n",
        "clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the clusters\n",
        "\n",
        "We can dimensionality-reduce our tf-idf vectors and plot them in order to see whether the clusters roughly align with the data, like so:"
      ],
      "metadata": {
        "id": "yFItknP0XMnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# recall that each film synopsis is a tf_idf vector in the list of tfidf_vectorizer_vectors\n",
        "\n",
        "# now convert tfidf_vectorizer_vectors to a dense array for plotting\n",
        "tfidf_vectorizer_vectors_dense = tfidf_vectorizer_vectors.toarray()\n",
        "\n",
        "# reduce dimensionality for visualization using PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "tfidf_2d = pca.fit_transform(tfidf_vectorizer_vectors_dense)\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(tfidf_2d[:, 0], tfidf_2d[:, 1], c=clusters)\n",
        "plt.title('Film synopses clustered by k-means')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "\n",
        "# create a custom legend\n",
        "legend_elements = []\n",
        "for i in range(num_clusters):\n",
        "  legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', label=f'Cluster {i}', markerfacecolor=plt.cm.get_cmap('viridis', num_clusters)(i), markersize=10))\n",
        "\n",
        "plt.legend(handles=legend_elements, title=\"Clusters\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bo5t9J_jYyDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hm... At least looking at the first two components, these films don't seem very cluster-able, at least on the basis of their synopses. Let's just inspect the fourth cluster since that one seems the most distinct of the bunch.  "
      ],
      "metadata": {
        "id": "fDdFwRNa0HAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting clusters\n",
        "\n",
        "First we should dump our clusters into a dataframe for better inspection and manipulation"
      ],
      "metadata": {
        "id": "p32fEauBjFS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHQoPX50ZD2q"
      },
      "outputs": [],
      "source": [
        "films = { 'title': titles, 'idx': list(range(100)), 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
        "\n",
        "film_df = pd.DataFrame(films, columns = ['title', 'idx', 'cluster', 'genre'])\n",
        "\n",
        "film_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can inspect the clusters a bit more easily"
      ],
      "metadata": {
        "id": "SJxqM-LK3dLU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY2a95xYZD2r"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "# find all films in each cluster\n",
        "for i in range(num_clusters):\n",
        "    print(\"Titles in cluster \" + str(i) + \": \")\n",
        "    cluster_titles = \"\"\n",
        "\n",
        "    # create new df of only the specific cluster\n",
        "    # remember boolean selection!\n",
        "    cluster_df = film_df[ film_df[\"cluster\"] == i ]\n",
        "\n",
        "    # create series of titles assoc w/ that cluster\n",
        "    for title in cluster_df['title']:\n",
        "        cluster_titles += title + \", \"\n",
        "\n",
        "    print(textwrap.fill(cluster_titles, 100) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get some more insight, we could visualize the clusters by genre rather than by cluster. Let's do that now.\n",
        "\n",
        "First, we need to add our PCA components to our dataframe:"
      ],
      "metadata": {
        "id": "QD4pmZqu3j0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add tfidf_2d to the dataframe in columns 'pca1' and 'pca2'\n",
        "film_df['pca1'] = tfidf_2d[:, 0]\n",
        "film_df['pca2'] = tfidf_2d[:, 1]\n",
        "\n",
        "film_df"
      ],
      "metadata": {
        "id": "tBOIAJxIahHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we still need to clean up the genres a bit more.\n",
        "\n",
        "Regex to the rescue!\n",
        "\n"
      ],
      "metadata": {
        "id": "GoJ4EP_gjJQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "cleaned_genres = []\n",
        "\n",
        "for genre_string in film_df['genre']:\n",
        "  cleaned_genre_string = re.sub(r\"u'\", '', genre_string) # clean the u' from the start of each genre\n",
        "  cleaner_genre_string = re.sub(\"[\\[\\]\\s\\']+\", \"\", cleaned_genre_string) # remove [, ], ', and whitespace\n",
        "  cleaner_genre_list = cleaner_genre_string.split(',') # split into a list\n",
        "  cleaned_genres.append(cleaner_genre_list)\n",
        "\n",
        "# reassign to the genre column\n",
        "film_df['genre'] = cleaned_genres\n",
        "\n",
        "# check it out\n",
        "film_df"
      ],
      "metadata": {
        "id": "Wtf40CWOdIlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can plot the films according to genre"
      ],
      "metadata": {
        "id": "WhT4wwW7kr-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "\n",
        "# Create a dictionary to map genres to colors\n",
        "unique_genres = set()\n",
        "for genres_list in film_df['genre']:\n",
        "    for genre in genres_list:\n",
        "        unique_genres.add(genre)\n",
        "\n",
        "# Generate a colormap with a unique color for each genre\n",
        "genre_colors = dict(zip(unique_genres, plt.cm.get_cmap('tab20', len(unique_genres))(range(len(unique_genres)))))\n",
        "\n",
        "# Function to blend colors based on multiple genres\n",
        "def blend_colors(genres_list):\n",
        "    if len(genres_list) == 0 :\n",
        "      return (0,0,0,0) # handle cases where a movie has no listed genres.\n",
        "\n",
        "    r, g, b, a = 0, 0, 0, 0\n",
        "    for genre in genres_list:\n",
        "        if genre in genre_colors:\n",
        "          cr, cg, cb, ca = genre_colors[genre]\n",
        "          r += cr\n",
        "          g += cg\n",
        "          b += cb\n",
        "          a += ca\n",
        "        else:\n",
        "          r += 0\n",
        "          g += 0\n",
        "          b += 0\n",
        "          a += 0\n",
        "\n",
        "    return (r / len(genres_list), g / len(genres_list), b / len(genres_list), a / len(genres_list))\n",
        "\n",
        "# Create a list of blended colors for each movie\n",
        "blended_colors = [blend_colors(genres_list) for genres_list in film_df['genre']]\n",
        "\n",
        "# Plot PCA1 and PCA2 with blended colors\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(film_df['pca1'], film_df['pca2'], c=blended_colors)\n",
        "\n",
        "plt.title('Film Synopses Clustered by k-means (Colored by Genre)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "\n",
        "# Create the custom legend\n",
        "legend_elements = []\n",
        "for genre, color in genre_colors.items():\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', label=genre, markerfacecolor=color, markersize=10))\n",
        "\n",
        "plt.legend(handles=legend_elements, title=\"Genres\", bbox_to_anchor=(1.05, 1), loc='upper left')  # Adjust legend position\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L-RrUODFdTDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same thing as above, just with boundary lines to see the clusters"
      ],
      "metadata": {
        "id": "jVhpWbJpkjC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: modify the code above to also draw a boundary line around each cluster\n",
        "\n",
        "# ... (Your existing code)\n",
        "\n",
        "# Assuming film_df is already created as in your provided code\n",
        "\n",
        "# Create a dictionary to map genres to colors\n",
        "unique_genres = set()\n",
        "for genres_list in film_df['genre']:\n",
        "    for genre in genres_list:\n",
        "        unique_genres.add(genre)\n",
        "\n",
        "# Generate a colormap with a unique color for each genre\n",
        "genre_colors = dict(zip(unique_genres, plt.cm.get_cmap('tab20', len(unique_genres))(range(len(unique_genres)))))\n",
        "\n",
        "# Function to blend colors based on multiple genres\n",
        "def blend_colors(genres_list):\n",
        "    if len(genres_list) == 0 :\n",
        "      return (0,0,0,0) # handle cases where a movie has no listed genres.\n",
        "\n",
        "    r, g, b, a = 0, 0, 0, 0\n",
        "    for genre in genres_list:\n",
        "        if genre in genre_colors:\n",
        "          cr, cg, cb, ca = genre_colors[genre]\n",
        "          r += cr\n",
        "          g += cg\n",
        "          b += cb\n",
        "          a += ca\n",
        "        else:\n",
        "          r += 0\n",
        "          g += 0\n",
        "          b += 0\n",
        "          a += 0\n",
        "\n",
        "    return (r / len(genres_list), g / len(genres_list), b / len(genres_list), a / len(genres_list))\n",
        "\n",
        "# Create a list of blended colors for each movie\n",
        "blended_colors = [blend_colors(genres_list) for genres_list in film_df['genre']]\n",
        "\n",
        "\n",
        "# Plot PCA1 and PCA2 with blended colors and cluster boundaries\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot data points\n",
        "plt.scatter(film_df['pca1'], film_df['pca2'], c=blended_colors)\n",
        "\n",
        "# new part!\n",
        "# Calculate cluster boundaries using convex hull\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "for cluster_id in range(num_clusters):\n",
        "    cluster_points = tfidf_2d[clusters == cluster_id]\n",
        "    hull = ConvexHull(cluster_points)\n",
        "    for simplex in hull.simplices:\n",
        "        plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1], 'k-')\n",
        "\n",
        "\n",
        "plt.title('Film Synopses Clustered by k-means (Colored by Genre, Boundaries)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "\n",
        "# Create the custom legend\n",
        "legend_elements = []\n",
        "for genre, color in genre_colors.items():\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', label=genre, markerfacecolor=color, markersize=10))\n",
        "\n",
        "plt.legend(handles=legend_elements, title=\"Genres\", bbox_to_anchor=(1.05, 1), loc='upper left')  # Adjust legend position\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HoEvA1fDkMWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OElgtonZD2r"
      },
      "source": [
        "OK. Still not convincing. On to evaluation.\n",
        "\n",
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting K\n",
        "So, the question remains: is this the right number for k?\n",
        "\n",
        "One of the metrics commonly used to compare results across different values of k is called \"inertia,\" which is a measure of within-cluster variance.\n",
        "\n",
        "More specifically, inertia measures the sum of the squared distances between each datapoint and the nearest cluster center (or \"centroid\").\n",
        "\n",
        "Lower inertia values indicate that points are closer to their assigned cluster centers, implying more compact clusters.\n",
        "\n",
        "However, since increasing the number of clusters will always decrease the inertia, since the more clusters you have the closer the points can be to its center, inertia cannot be used as a standalone measure.\n",
        "\n",
        "Instead, mean distance to the centroid as a function of k is plotted and the \"elbow point,\" where the rate of decrease sharply shifts, can be used to roughly determine the ideal value for k.\n",
        "\n",
        "Some other techniques exist for validating k, but that's the most common one.\n",
        "\n",
        "Let's try it!"
      ],
      "metadata": {
        "id": "BI7rHV2JT-aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inertia_vals = []\n",
        "\n",
        "for i in range(1, 21):\n",
        "    km_clustering = KMeans(n_clusters=i, n_init=10)\n",
        "    km_clustering.fit(tfidf_vectorizer_vectors)\n",
        "    inertia_vals.append(km_clustering.inertia_)\n",
        "\n",
        "ks = list(range(1, 21))\n"
      ],
      "metadata": {
        "id": "TBl_WHNuuIOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.xlabel('number of clusters k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.plot(ks, inertia_vals, 'bx-')\n"
      ],
      "metadata": {
        "id": "LLaRvWNMy9Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# library for quickly calculating \"knee\"\n",
        "%pip install kneed"
      ],
      "metadata": {
        "id": "EhpfOA9Vyley"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kneed import KneeLocator\n",
        "kn = KneeLocator(ks, inertia_list, curve='convex', direction='decreasing')\n",
        "print(kn.knee)"
      ],
      "metadata": {
        "id": "aMLzzaSSx3_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of clusters k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.plot(ks, inertia_list, 'bx-')\n",
        "plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')"
      ],
      "metadata": {
        "id": "oRa1K8OSuIMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMg-kyNcZD2r"
      },
      "source": [
        "# A few miscellaneous things...\n",
        "\n",
        "Finding top terms per cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AwP2si9ZD2q"
      },
      "outputs": [],
      "source": [
        "# find the top terms per cluster\n",
        "\n",
        "# this orders by the distance of each term from the center\n",
        "# (cluster_centers_ returns an array of [n_clusters, n_features] )\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster \" + str(i) + \" top words: \")\n",
        "    top_terms = \"\"\n",
        "\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        top_terms += terms[ind] + \", \"\n",
        "\n",
        "    print(top_terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find top 10 films in each cluster"
      ],
      "metadata": {
        "id": "VK9aB69eLyGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnF_o5elZD2r"
      },
      "outputs": [],
      "source": [
        "# Find the top 10 films in each cluster\n",
        "for i in range(num_clusters):\n",
        "\n",
        "    # returns array w/ distances to the centroid i\n",
        "    dist = km.transform(tfidf_vectorizer_vectors)[:, i]\n",
        "\n",
        "    # return indices for top 10 for that cluster\n",
        "    idxs = np.argsort(dist)[::][:10]\n",
        "\n",
        "    print(\"Top 10 films in cluster \" + str(i) + \": \")\n",
        "    cluster_top_films = \"\"\n",
        "\n",
        "    # look up film title by index\n",
        "    for idx in idxs:\n",
        "        cluster_top_films += film_df.loc[idx,'title'] + \", \"\n",
        "\n",
        "    print(cluster_top_films + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Based on materials by Brandon Rose, Jacob Eisenstein, and Eun Seo Jo et al.*\n"
      ],
      "metadata": {
        "id": "i8L-5tHyypt7"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}