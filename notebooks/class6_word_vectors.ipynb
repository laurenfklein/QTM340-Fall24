{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EC7T4CoW6IQ"
      },
      "source": [
        "# Word Embedding Models: word2vec #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZp_w5tMW6IU"
      },
      "source": [
        "![oprah vector](http://lklein.com/wp-content/uploads/2021/10/oprah-everyone-3.png)\n",
        "\n",
        "### Everything gets a vector! ###\n",
        "\n",
        "We've already been exploring vectors involving words: consider scikit-learn's `CountVectorizer()`, for example, which we used to create the document-term matrix for our tf-idf calculations. That looked at words in relation to the documents in which they appeared.\n",
        "\n",
        "Today, however, we're going to look at words in relation to all other words in a corpus. The vectors that describe these types of relations are called, appropriately enough, *word vectors*. (And sometimes also *word embeddings*).\n",
        "\n",
        "### What is a word vector? ###\n",
        "\n",
        "A *word vector* or *word embedding* is a numerical representation of a word within a corpus, based on co-occurence with other words. Linguists have found that much of the meaning of a word can be derived from looking at the context in which it appears. (In linguistics, this is known as the theory of *distributional semantics*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFNdVZ_DW6IV"
      },
      "source": [
        "### What is Word2Vec? ###\n",
        "\n",
        "Word2vec is one popular approach to representing words in this numerical format. Conveniently, word2vec is implemented in a library called `gensim`, which we will also use later in the semester for topic modeling.\n",
        "\n",
        "Word2Vec is a *neural-network* or *deep learning* based approach of generating word vectors.\n",
        "\n",
        "There are many resources out there that will go into the heavy details of deep learning in general or deep learning for NLP such as Yoav Goldberg's Neural Network Methods in Natural Language Processing (Morgan & Claypool Publishers, 2017). Today, you'll get a high level overview -- just enough for you to understand what w2v is doing.\n",
        "\n",
        "### A Picture for Reference ###\n",
        "\n",
        "Before we get into the details of neural networks and deep learning, let's take a quick look at an image that may help anchor some of the more heady concepts we're about to discuss. This shows us the word pairs for a tiny corpus, consisting of a single sentence, \"The quick brown fox jumps over the lazy dog.”\n",
        "\n",
        "The words pairs from this sentence will constitute our training data: what we will use to generate our word vectors. I’ve used a small window size of 2 just for the example. Most of the time the window size will be slightly longer, like 5. In any case, the word highlighted in blue is the input word.\n",
        "\n",
        "![skip-grams](http://mccormickml.com/assets/word2vec/training_data.png)\n",
        "\n",
        "The neural network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“brown”, “fox”) than it is of (“brown”, “unicorn”). When the training is finished, if you give it the word “brown” as input, then it will output a much higher probability for “fox” or “bear” than it will for “unicorn”.\n",
        "\n",
        "But how are these probabilities generated, and what is a neural network anyway? Let's take a minute to talk through these ideas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4oICczFW6IW"
      },
      "source": [
        "### What is a neural network? ###\n",
        "\n",
        "Here's great explainer adapted from Jer Thorp's recent book, [Living in Data](https://www.jerthorp.com/)\n",
        "\n",
        "\"A neural network is a mathematical model that traces its roots back to the 1940s, when logicians and neuroscientists and cyberneticians were trying to explain how the human brain learns. Taking a page from how actual neurons cells work, neural net pioneers conceived of a system of nodes, each a kind of simplified neuron. These nodes hold a number, called an activation potential, above which a node will \"fire,\" sending a signal to one or more other nodes; or, if there is no node to talk to, spitting out a result. Stitched together into collections (networks), these nodes showed the ability to recognize pattern; that is, to take a specific set of numeric inputs and to turn it, consistently, into an expected result.\"\n",
        "\n",
        "\"Imagine a group of thirteen children in a classroom, sitting in three neat rows of four, with one sitting alone in the back row. Each child can be given either a coookie or a nap. A cookie increases the kid's energy level by one; a nap reduces it by one. If a child's energy goes above a level ten, they have a tantrum, exhausting their excitement but also passing some of it on to any kids they may be connected to. Neural networks tend to be \"feed-forward\" meaning that the signal can only go in one direction from node to node. In our classroom, we can take this to mean that kids can pass energy back only to those sitting behind them.\"\n",
        "\n",
        "\"If we feed a plate of cookies to the kids in the front row, we can expect a wave of hysterics to pass from the front to the back of the class, ending with our lonely back-row student in tears. If every child in the front row got the same amount of sugar, and if they all had the same tolerance for it, this wave would be uniform, starting and ending with crying kids. Neural networks function the way they do, though, because the nodes aren't uniform; they are weighted. This meanas eveyr kid in our class has a different tolerance for cookies, a different level at which they'll break into a conniption. The wave of tears won't flow evenly from front to back, and the signal that we pass into the front won't be the same as the one that comes out the back.\"\n",
        "\n",
        "\"Asuming the kids in the class are randomly weighted, each with a unique combination of patience and metabolism, feeding different numbers of cookies to the four kids in the front row would result in times when the back-row student loses their temper and times when they don't. Importantly, feeding the same pattern of cookies to the front of the class will always result in the same outcome in the back. This means that the classroom acts together as a pattern-recognition machine. Anything we might be able to translate into 'cookie language,\" a set of four numbers, can be fed into the machine to get a tantrum-based yes or no.\"\n",
        "\n",
        "If the teacher wanted to make sure they got a specific reation from a specific piece of cookie code, they could reseat or replace the students, feeding the cookies to the front until the teacher saw the answer they wanted from the back. The teacher might train the class to recognize their birth year--2015--or the first four notes in \"Baby Shark,\" or a binary representation for the number twelve. In a school assembly, with many more students, this same system could be arranged to recognize bigger sets of numbers, digitized words, or pixelated faces. More than that, a large network might be trained to recognize signals that are similar: faces that are smiling or words that rhyme with \"cheese.\" A crucial point here is that the kids in the network don't need to know anything about the signal or the desired output. They just eat cookies, cry, nap, and compute.\"\n",
        "\n",
        "Of course, we're not training a kids-and-cookies neural network. We're training a computational one with a far greater number of nodes.\n",
        "\n",
        "The important takeaway here, aside from how neural networks go from input to output, is that neural networks are not algorithms in themselves. They just go from start to finish. In order to make use of the input and output, they are most usually paired with algorithms that *train* the network, improving its performance over multiple iterations.\n",
        "\n",
        "Which brings us back to the word2vec algorithm and how it trains the neural network at its core."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56JGDiy7W6IY"
      },
      "source": [
        "### Training the neural network ###\n",
        "\n",
        "Here, we’re going to train our neural network to do something more complicated than predict whether a kid at the back of the room will cry or not. Our task is this: given a specific word in the middle of a sentence (the input word--like \"brown,\" as in the image above), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.\n",
        "\n",
        "Let's look at our image again:\n",
        "\n",
        "![skip-grams](http://mccormickml.com/assets/word2vec/training_data.png)\n",
        "\n",
        "So \"nearby\" is actually defined by the \"window size\" parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total). But in the image the window size is 2.\n",
        "\n",
        "We train the neural network to do this by feeding these word pairs, and he neural network is going to learn the statistics from the number of times each pairing shows up.\n",
        "\n",
        "**NOTE:** I've described something called the skim-gram methods of generating word vectors. THere's also another popular method called CBOW (continuous bag of words). The main difference is that while skip gram learns vectors by predicting the context words that come before and after our given word $w$, CBOW predicts the center word $w$ given context words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf7oZ9JjW6IZ"
      },
      "source": [
        "### One more wrinkle / bonus  ###\n",
        "\n",
        "One last wrinkle in the word2vec process is that, in the end, we’re not actually interested in the predictions generated by the model. What we're interested in is the weights of the nodes of the network itself. These are the actual \"word vectors\" that we want to work with.\n",
        "\n",
        "We can access them fairly easily because word2vec has only a single hidden (or \"projection\") layer, as displayed in the image below.\n",
        "\n",
        "![neural network](http://lklein.com/wp-content/uploads/2019/10/mikolov.png)\n",
        "\n",
        "Conveniently, all you need is one hidden layer for a neural network to be classified as a \"deep\" network. So we're doing deep learning! Fancy!  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq5iLgi8W6Ia"
      },
      "source": [
        "# Let's try it out!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rknMC_0KW6Ia"
      },
      "source": [
        "## Import gensim, nltk tokenizers, glob, and Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpPQx_0aW6Ib"
      },
      "outputs": [],
      "source": [
        "import gensim # remember this from last class?\n",
        "\n",
        "# and some other stuff\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import glob\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load in our corpus"
      ],
      "metadata": {
        "id": "I3C9gP1c7iUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading large files from Google Drive\n",
        "# https://github.com/wkentaro/gdown\n",
        "import gdown\n",
        "\n",
        "# then download the zip files\n",
        "# atlanta\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1gIm9NcoeY1gn9EQjRr2MojGRJ-fpBSqz', quiet=False)\n",
        "\n",
        "# unzip it\n",
        "!unzip Atlanta-random.jsonl.zip"
      ],
      "metadata": {
        "id": "suz2F1GJ7hSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkIgr1tIW6Id"
      },
      "source": [
        "### Process the docs\n",
        "\n",
        "As a first step, we'll need to create a list of all the reviews in the `Atlanta-random.jsonl` file, with each review stored as a single string. This is the same exact code we used last class, but condensed a little."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3539wU8PW6Ie"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os             # for directory/file manipulation\n",
        "import json           # for json\n",
        "import pandas as pd   # for dataframes\n",
        "import textwrap       # for nice formatting\n",
        "\n",
        "# read in the file\n",
        "atlanta_reviews_df = pd.read_json(path_or_buf=\"./Atlanta-random.jsonl\", lines=True)\n",
        "\n",
        "# first extract the 'comment' values from the dataframe\n",
        "comments = atlanta_reviews_df['comment'].tolist()\n",
        "\n",
        "# create list to store reviews\n",
        "all_reviews = []\n",
        "\n",
        "# iterate through the comments and append the reviews to the list\n",
        "for comment in comments:\n",
        "  all_reviews.append(comment['text'])\n",
        "\n",
        "# print out the length just to check that everything got in\n",
        "len(all_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We learned last class that there were some extra HTML tags embedded in the review text, and we also saw some hex codes. Let's see if we can clean things up a little before we move further."
      ],
      "metadata": {
        "id": "NOKPENcxCCSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# new array w/ clean text\n",
        "all_reviews_clean = []\n",
        "\n",
        "for review in all_reviews:\n",
        "    soup = BeautifulSoup(review, \"html.parser\")\n",
        "    text = soup.get_text(separator=' ')\n",
        "\n",
        "    all_reviews_clean.append(text)\n",
        "\n",
        "# print out first one just to check\n",
        "print(textwrap.fill(all_reviews_clean[0], 100))"
      ],
      "metadata": {
        "id": "G4jP9FnyC8Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One last thing before we get started. Let's create some legible IDs for each revivew using some other info that's in the dataframe."
      ],
      "metadata": {
        "id": "lieFma4XI_wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the 'business' values from the dataframe\n",
        "businesses = atlanta_reviews_df['business'].tolist()\n",
        "\n",
        "# create list to store business aliases\n",
        "aliases = []\n",
        "\n",
        "# iterate through the business and append the alias to the list\n",
        "for business in businesses:\n",
        "  aliases.append(business['alias'])\n",
        "\n",
        "# extract ratings\n",
        "ratings = atlanta_reviews_df['rating'].tolist()\n",
        "\n",
        "# create list to store IDs <-- we'll use list this going forward\n",
        "ids = []\n",
        "\n",
        "# now put them all together into IDs\n",
        "for i, alias in enumerate(aliases):\n",
        "  id = alias + \"-review\" + str(i) + \"-\" + str(ratings[i]) + \"stars\"\n",
        "  ids.append(id)\n",
        "\n",
        "# print out the first one to check\n",
        "ids[0]"
      ],
      "metadata": {
        "id": "uX3QepfxJJVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xItKJ_tRW6Ie"
      },
      "source": [
        "Next, we need to get each of the docs in our `all_reviews_clean` list into the format required by gensim's implementation of word2vec.\n",
        "\n",
        "We know from the gensim documentation (and also common sense) that the input to `word2vec` is sentences. So let's define a function that a takes a list of texts (e.g. our `all_reviews_clean::` list) and converts it into sentences for gensim word2vec to use. The function will lower-case text and tokenize by sentence and word. It will also print out a count of the sentences in each doc, so that we get some sort of status indicator that it's parsing the sentences correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgCyM8oHW6If"
      },
      "outputs": [],
      "source": [
        "# need our handy nltk tokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "# and the function\n",
        "def make_sentences(list_txt):\n",
        "    all_txt = []\n",
        "    counter = 0\n",
        "    for txt in list_txt:\n",
        "        lower_txt = txt.lower()\n",
        "        sentences = sent_tokenize(lower_txt)\n",
        "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "        all_txt += sentences\n",
        "        print(ids[counter]) # let's print the title of the article\n",
        "        print(\"Sentences: \" + str(len(sentences)))  # let's check how many sentences there are per article\n",
        "        counter += 1\n",
        "    return all_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnAeHkMuW6If"
      },
      "outputs": [],
      "source": [
        "# now let's run it\n",
        "sentences = make_sentences(all_reviews_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvt6XI3GW6Ig"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk1WAuZ4W6Ig"
      },
      "source": [
        "Now that we have our corpus ready for gensim, we can train the model. To do so, we call the function `gensim.models.Word2Vec()`. This function has a couple dozen parameters, some of which are more important than others.\n",
        "\n",
        "Here are a few major ones. Only two are MANDATORY: these are marked with an asterisk:\n",
        "\n",
        "1. `sentences*`: This is where you provide your data. It must be in a format of iterable of iterables.\n",
        "2. `sg`: Your choice of training algorithm. There are two standard ways of training W2V vectors -- 'skipgram' and 'CBOW'. If you enter 1 here the skip-gram is applied; otherwise, the default is CBOW.\n",
        "3. `size*`: This is the length of your resulting word vectors. If you have a large corpus (>few billion tokens) you can go up to 100-300 dimensions. Generally word vectors with more dimensions give better results.\n",
        "4. `window`: This is the window of context words you are training on. In other words, how many words come before and after your given word. A good number is 4 here but this can vary depending on what you are interested in. For instance, if you are more interested in embeddings that embody semantic meaning, smaller window sizes work better.\n",
        "5. `alpha`: The learning rate of your model. If you are interested in machine learning experimentation with your vectors you may experiment with this parameter.\n",
        "6. `seed` (int): This is the random seed for your random initialization. All deep learning models initialize the weights with random floats before training. This is a useful field if you want to replicate your experiments because giving this a seed will initialize 'randomly' deterministically.\n",
        "7. `min_count`: This is the minimum frequency threshold. If a given word appears with lower frequency than provided it will be ignored. This is here because words with very low frequency are hard to train.\n",
        "8. `epochs`: This is the number of iterations (entire run) over the corpus, also known as epochs. Default is 5. Usually anything between 1-10 is ok. The trade offs are that if you have higher iterations, it will take longer to train and the model may overfit on your dataset. However, longer training will allow your vectors to perform better on tasks relevant to your dataset.\n",
        "\n",
        "Most of these settings will not concern us. As you'll see below, we are only going to use four arguments.\n",
        "\n",
        "\\* On newer versions of gensim, `size` has changed to `vector_size`. But Colab has not yet updated theirs so `size` still works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJlVgtD6W6Ig"
      },
      "outputs": [],
      "source": [
        "# let's train our model!\n",
        "atl_reviews_model = gensim.models.Word2Vec(\n",
        "    sentences,\n",
        "    min_count=2, # default is 5; this trims the corpus for words only used once;\n",
        "    vector_size=100,\n",
        "    workers=5) # parallel processing; needs Cython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9U7J3EJW6Ig"
      },
      "source": [
        "Hooray! We have a trained word2vec model: `atl_reviews_model`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbMZyxr3W6Ig"
      },
      "source": [
        "### Save model — and load it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPwEfuFRW6Ih"
      },
      "source": [
        "It's often useful to save your trained model to disk so that you can reload it as needed. This is very similar syntax to saving topic models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfuUFv3pW6Ih"
      },
      "outputs": [],
      "source": [
        "# how to store the above files to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "atl_reviews_model.save('/content/gdrive/My Drive/atl_reviews_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xYGUHTiW6Ih"
      },
      "source": [
        "And you can load an old model in the same way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYSx8IfJW6Ih"
      },
      "outputs": [],
      "source": [
        "# how you would load an old model from your own google drive\n",
        "old_model = gensim.models.Word2Vec.load('/content/gdrive/My Drive/atl_reviews_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSpjUL_YW6Ih"
      },
      "source": [
        "## Let's play!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hom5bGa8W6Ii"
      },
      "source": [
        "### Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BieUbIonW6Ii"
      },
      "source": [
        "word2vec can tell us which words, according to its model, are most similar to any other. We call `model.wv.most_similar(\"word\", topn=number of similar words)`. Let's try \"delicious.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AowqURgWW6Ii"
      },
      "outputs": [],
      "source": [
        "# testing some basic functions\n",
        "\n",
        "# basic similarity w/ adjectives\n",
        "atl_reviews_model.wv.most_similar(\"delicious\", topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic similarity w/ nouns\n",
        "atl_reviews_model.wv.most_similar(\"biscuit\", topn=10)"
      ],
      "metadata": {
        "id": "b18FZVngXjmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlz--aW8W6Ii"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89WVCZjdW6Ii"
      },
      "source": [
        "## Exercise 1:\n",
        "\n",
        "**Copy the code above and test out some words until you find one that has some interesting (or problematic) similar words.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wurp5alIW6Ii"
      },
      "outputs": [],
      "source": [
        "# your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlckdpWEW6Ij"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLC0p3SuW6Ij"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5QlDf5NW6Ij"
      },
      "source": [
        "## Similarity between two words\n",
        "\n",
        "We can choose specific words to compare with `model.wv.similarity(w1=\"word_one\",w2=\"word_two\")`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbX4gqGwW6Ij"
      },
      "outputs": [],
      "source": [
        "# similarity b/t two words\n",
        "\n",
        "print(atl_reviews_model.wv.similarity(w1=\"meatballs\",w2=\"delicious\"))\n",
        "print(atl_reviews_model.wv.similarity(w1=\"meatballs\",w2=\"disgusting\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB5CbtsmW6Ij"
      },
      "source": [
        "As expected (meatball stan here), more reviews in the model find meatballs delcious than disguisting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAKptkksW6Ij"
      },
      "source": [
        "### Analogy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJXeEj42W6Ik"
      },
      "source": [
        "We can also play with analogy tasks. The commonly seen task is:\n",
        "\n",
        "'Man is to King as Woman is to ____?'\n",
        "\n",
        "The general structure is:\n",
        "`A is to A\\*  as  B is to B\\*`\n",
        "                         \n",
        "gensim provides two different ways of implementing this task. You may be familiar with the the additive version also called the 3CosAdd method:\n",
        "\n",
        "$$\\underset{b*\\in V}{\\textrm{arg max}} (cos(b*,b) - cos(b*,a) + cos(b*,a*))$$\n",
        "\n",
        "This reflects the abstraction of Woman - Man + King. In this maximization, we are searching which word vector will allow us to produce the highest value in this equation.\n",
        "\n",
        "We can implement this method with a built-in function. Positive here refers to words that give the positive contribution to similarity (nominator), and negative refers to words that contribute negatively (denominatory).\n",
        "\n",
        "Or, in simpler language, you can also think of this as, \"start at 'professor'-vector, add 'she'-vector, subtract ‘he'-vector, from where you wind up, report the top-ranked word-vectors closest to that point (not including any of the 3 query vectors).\n",
        "\n",
        "Here it is:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# analogies\n",
        "# format is: \"man is to king as woman is to ???\"\n",
        "\n",
        "result = atl_reviews_model.wv.most_similar(positive=['knowledgable', 'man'], negative=['woman'])\n",
        "\n",
        "print(\"{}: {:.4f}\".format(*result[0])) # this prints the top result"
      ],
      "metadata": {
        "id": "2xyuoBvcaDeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-P8EzvfW6Ik"
      },
      "outputs": [],
      "source": [
        "# analogies\n",
        "# format is: \"man is to king as woman is to ???\"\n",
        "\n",
        "result = atl_reviews_model.wv.most_similar(positive=['authentic', 'italian'], negative=['chinese'])\n",
        "\n",
        "print(\"{}: {:.4f}\".format(*result[0])) # this prints the top result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFV0TxDxW6Ik"
      },
      "source": [
        "## Exercise 2:\n",
        "\n",
        "**Copy the code above and test out some analogies until you find one that gets you some interesting (or problematic) results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uop6lEtW6Ik"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfqf_cV4W6Il"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7CvOYB1W6Il"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0_q8O8hW6Il"
      },
      "source": [
        "## There's so much more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBBRWRO-W6Il"
      },
      "source": [
        "gensim has quite a few built-in tools, and it's worth taking some time to see what's available. Check the documentation here: [https://radimrehurek.com/gensim/models/keyedvectors.html](https://radimrehurek.com/gensim/models/keyedvectors.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg0KciXaW6Il"
      },
      "source": [
        "## BONUS: Visualization!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5TnNM1W6Il"
      },
      "source": [
        "Find below some code you can use to make visualizations from your word2vec model. We can't visualize all the many dimensions in our model, so we need to reduce them to two dimensions for our meager human brains. We do that with something called principal component analysis (PCA).\n",
        "\n",
        "Don't worry about the details for now. This is just a fun way to take a look at the output of our model.\n",
        "\n",
        "**Remember**: Our visualization reduces MANY dimensions to two, so a lot of information is lost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsnLxBViW6Im"
      },
      "outputs": [],
      "source": [
        "### Let's do some visualization ###\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Get the interactive Tools for Matplotlib\n",
        "# %matplotlib notebook # doesn't work for colab\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ93BG_BW6Im"
      },
      "outputs": [],
      "source": [
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.wv.vocab.keys()), sample)\n",
        "        else:\n",
        "            words = [ word for word in model.wv.vocab ]\n",
        "\n",
        "#    word_vectors = np.array([model[w] for w in words]) <-- gensim 3 version\n",
        "    word_vectors = np.array([model.wv[w] for w in words]) # gensim 4 version\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b8nacTDW6Im"
      },
      "outputs": [],
      "source": [
        "display_pca_scatterplot(atl_reviews_model, ['italian','french','american','korean','japanese','mexican','chinese'])\n",
        "\n",
        "# display_pca_scatterplot(ccp_model, sample=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBlX11k4W6In"
      },
      "source": [
        "## Exercise 3a:\n",
        "\n",
        "**Copy the code above and plot some words that you think might be similar or different from each other.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic0pnYuOW6In"
      },
      "outputs": [],
      "source": [
        "# your plot here"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4BhM-SMxvbqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXKBlXg8vbel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3b:\n",
        "\n",
        "**What do you think the plot shows you about the words? Did they confirm or contradict what you though they would show?**"
      ],
      "metadata": {
        "id": "3np3pEpFu41G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your answer here"
      ],
      "metadata": {
        "id": "YsC1sLgbvCwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RlO5zeBvdvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "610kiUqqvdj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Lauren F. Klein wrote version 1.0 of this notebook in 2019 based on the [Advanced Topics in Word Vectors workshop](https://dh2018.adho.org/en/machine-reading-part-ii-advanced-topics-in-word-vectors/) at DH 2018 as well as tutorials by [Radim Rehurek](https://rare-technologies.com/word2vec-tutorial/) and [Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). It was updated again in 2021, 2022, and 2024.*"
      ],
      "metadata": {
        "id": "LK8KcqKdXCpz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMAChygsXFE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}