{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URKn8K0HydhV"
      },
      "source": [
        "# Training and Fine-Tuning BERT for Classification\n",
        "## Classfying Goodreads Reviews By Book Genre\n",
        "\n",
        "This notebook will demonstrate how to train and fine-tune a BERT model for classification with the popular HuggingFace `transformers` Python library.\n",
        "\n",
        "We will fine-tune a BERT model on Goodreads reviews from the [UCSD Book Graph](https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/reviews?authuser=0) with the goal of predicting the genre of the book being reviewed. The genres include:\n",
        "- poetry\n",
        "- comics & graphic\n",
        "- fantasy & paranormal\n",
        "- history & biography\n",
        "- mystery, thriller, & crime\n",
        "- romance\n",
        "- young adult  \n",
        "\n",
        "**Basic steps involved in using BERT and HuggingFace:**\n",
        "1. Divide your data into training and test sets.\n",
        "2. Encode your data into a format BERT will understand.\n",
        "3. Combine your data and labels into datset objects.\n",
        "4. Load the pre-trained BERT model.\n",
        "5. Fine-tune the model using your training data.\n",
        "6. Predict new labels and evaluate performance on your test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y73xWUkpU1AC"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Import necessary Python libraries and modules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ9kmfrO3M_w"
      },
      "source": [
        "First, we will import necessary Python libraries and modules. These include as `gdown`, for downloading large files from Google Drive (where we will get our UCSD Goodreads reviews), as well as the now-familiar scikit-learn (`sklearn`) and the less familiar (to us) PyTorch (`torch`), which allows us to optimize otherwise GPU/CPU-intensive deep learning methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9Si6kIWcULv"
      },
      "source": [
        "# Basic Python modules\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "# For downloading large files from Google Drive\n",
        "# Note that if you end up adapting this notebeook (or the next) for\n",
        "# your final projects, you'll likely need this to pull your data in\n",
        "# https://github.com/wkentaro/gdown\n",
        "import gdown\n",
        "\n",
        "# For working with gzip files\n",
        "# https://docs.python.org/3/library/gzip.html\n",
        "import gzip\n",
        "\n",
        "# For working with JSON files\n",
        "import json\n",
        "\n",
        "# For data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For machine learning tools and evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# For deep learning\n",
        "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "import torch\n",
        "\n",
        "# For plotting and data visualization\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import ticker\n",
        "sns.set(style='ticks', font_scale=1.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f29CPpKi3__Q"
      },
      "source": [
        "Import `DistilBert` modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ARMrKfmceAb"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q70KYS_NVSDL"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Set parameters and file paths**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piWsW9ZeaP_D"
      },
      "source": [
        "# This is the name of the BERT model that we want to use.\n",
        "# We're using DistilBERT to save space (it's a distilled version of the full BERT model),\n",
        "# and we're going to use the cased (vs uncased) version.\n",
        "model_name = 'distilbert-base-cased'\n",
        "\n",
        "# This is the name of the program management system for NVIDIA GPUs. We're going to send our code here.\n",
        "device_name = 'cuda'\n",
        "\n",
        "# This is the maximum number of tokens in any document sent to BERT.\n",
        "max_length = 512\n",
        "\n",
        "# This is the name of the directory where we'll save our model. You can name it whatever you want.\n",
        "cached_model_directory_name = 'distilbert-reviews-genres'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUjeaqiTehPY"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Load and sample Goodreads data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaYqqj7N8aRs"
      },
      "source": [
        "In this cell, we create a Python dictionary with each genre and the link to the corresponding UCSD Goodreads reviews zipfile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7pcelDZdrcC"
      },
      "source": [
        "# This is where our target data is hosted on the web. You only need these paths for the book review dataset.\n",
        "\n",
        "genre_url_dict = {'poetry':                 'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_poetry.json.gz',\n",
        "                  'children':               'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_children.json.gz',\n",
        "                  'comics_graphic':         'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_comics_graphic.json.gz',\n",
        "                  'fantasy_paranormal':     'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_fantasy_paranormal.json.gz',\n",
        "                  'history_biography':      'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_history_biography.json.gz',\n",
        "                  'mystery_thriller_crime': 'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_mystery_thriller_crime.json.gz',\n",
        "                  'romance':                'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_romance.json.gz',\n",
        "                  'young_adult':            'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_young_adult.json.gz'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZC5SadPAZEc"
      },
      "source": [
        "Next we loop through this dictionary and use `wget` to download the Goodreads review data for each genre and give it a shorter name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for genre, url in genre_url_dict.items():\n",
        "  !wget {url}"
      ],
      "metadata": {
        "id": "_RCmRhPBpdri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4E4d6C-AruL"
      },
      "source": [
        "Next we create a function `load_reviews()`, which will use `gzip` to unzip the downloaded Goodreads review JSON files and `json` to load the JSON files once they're unzipped.\n",
        "\n",
        "Note that if you end up adapting this code for your final project, your data will likely be in a different file format, so this is one of the functions that you'll need to adapt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfzAKfgKeuDv"
      },
      "source": [
        "def load_reviews(file_name,\n",
        "                 head=None):\n",
        "\n",
        "    reviews = []\n",
        "    count = 0\n",
        "\n",
        "    with gzip.open(file_name) as file:\n",
        "        for line in file:\n",
        "\n",
        "            d = json.loads(line)\n",
        "            count += 1\n",
        "\n",
        "            _book_id = d['book_id']\n",
        "\n",
        "            reviews.append(d['review_text'])\n",
        "\n",
        "            # Break if we reach the Nth line\n",
        "            if (head is not None) and (count > head):\n",
        "                break\n",
        "\n",
        "    return reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLVHBRGpBWvX"
      },
      "source": [
        "Now we apply the `load_reviews()` function. For each genre, we load and unzip the corresponding `.json.gz` file, e.g., `poetry.json.gz`, then we randomly sample 2000 Goodreads reviews and make a nested dictionary `genre_reviews_dict` of all these reviews.\n",
        "\n",
        "Here again, if you end up adaping this code, this cell will need to be modified depending on the format of your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8H_H-OmfgZ4"
      },
      "source": [
        "genres = ['poetry', 'children', 'comics_graphic', 'fantasy_paranormal', 'history_biography', 'mystery_thriller_crime', 'romance', 'young_adult']\n",
        "genre_reviews_dict = {}\n",
        "\n",
        "for _genre in genres:\n",
        "  print('Loading goodreads_reviews_' + _genre + '.json.gz')\n",
        "\n",
        "  _reviews = load_reviews('goodreads_reviews_' + _genre + '.json.gz')\n",
        "  genre_reviews_dict[_genre] = random.sample(_reviews, 2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdLLkJ9vZdYd"
      },
      "source": [
        "Let's just confirm that we've got 2000 reviews for each genre by looking at some components of the `genre_reviews_dict` dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQnOnrekaLh7"
      },
      "source": [
        "for _genre in genres:\n",
        "  print(\"Number of \" + _genre + \" reviews: \" + str(len(genre_reviews_dict[_genre])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hUYAM95IVyB"
      },
      "source": [
        "Let's also preview a random key-value pairs within each of the genres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf5RDXqGFQJy"
      },
      "source": [
        " for _genre, _reviews in genre_reviews_dict.items():\n",
        "    print(_genre)\n",
        "    print(random.sample(_reviews, 1)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJBuJ-7yFc9b"
      },
      "source": [
        "Now that we've confirmed that our data looks like we want it to, we can use `pickle` to save this Python dictionary to a `.pickle` file so we can easily load it later.\n",
        "\n",
        "*The `pickle` module allows you to save and load Python objects like lists and dictionaries.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnSpBCOuuKu-"
      },
      "source": [
        "pickle.dump(genre_reviews_dict, open('genre_reviews_dict.pickle', 'wb'))\n",
        "# genre_reviews_dict = pickle.load(open('genre_reviews_dict.pickle', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v10htL_G-OFs"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Split the data into training and test sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0qBs3MHGOSI"
      },
      "source": [
        "When training a machine learning model, it is necessary to split your training data into two parts: a \"training\" set and a \"test\" set. (Rule of thumb is 80% of the data for training, 20% for testing).\n",
        "\n",
        "In this case we will train our BERT model on the \"training\" set of Goodreads reviews and then we will evaluate how well it is performing by running it on the \"test\" set of Goodreads reviews that the model has never seen before.\n",
        "\n",
        "Normally, to tune the hyperparameters, you should also create a \"validation\" set for tuning, and only use the \"test\" set once, at the end of all tuning. (In that case, you'd want to divise your data into three categories: 80% training, 10% testing, and 10% validation). But for simplicity, in this lab, we will only using a training and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h94eg2wlfi6y"
      },
      "source": [
        "train_texts = []\n",
        "train_labels = []\n",
        "\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "\n",
        "for _genre, _reviews in genre_reviews_dict.items():\n",
        "\n",
        "  _reviews = random.sample(_reviews, 1000) # Use a very small set as an example.\n",
        "\n",
        "  for _review in _reviews[:800]:\n",
        "    train_texts.append(_review)\n",
        "    train_labels.append(_genre)\n",
        "  for _review in _reviews[800:]:\n",
        "    test_texts.append(_review)\n",
        "    test_labels.append(_genre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDqqi5tLAqG"
      },
      "source": [
        "Show how many Goodreads reviews and labels we have in each category: 6400 training reviews, 6400 training labels (genres), 1600 test reviews, 1600 test labels (genre)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tShbGfG-VMe"
      },
      "source": [
        "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-34oep8LNKw"
      },
      "source": [
        "Here's an example of a training label and review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcxvqUsdf5Sm"
      },
      "source": [
        "train_labels[0], train_texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FoaXKbKjXRX"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Run a baseline model (logistic regression)**\n",
        "\n",
        "Here we train and evaluate a simple TF-IDF baseline model using logistic regression. (Note the reapperance of TF-IDF!)\n",
        "\n",
        "We find better-than-random performance, even for a very small dataset. We'll see whether BERT can beat this good baseline!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBTyTh8Ui2D3"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMjmweu6MIQU"
      },
      "source": [
        "We train a logistic regression model from scikit-learn on the Goodreads training data, and then we use the trained model to make predictions on our Goodreads review test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R92S7JZfjiaC"
      },
      "source": [
        "model = LogisticRegression(max_iter=1000).fit(X_train, train_labels)\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQn6vXq4Gbb3"
      },
      "source": [
        "## **A Side-Note on Measures of Classification**\n",
        "\n",
        "The basic measure of classification prowess is accuracy: how often did classifier guess the class of a document correctly? This is calculated by simply dividing the number of correct guesses by the total number of documents considered.\n",
        "\n",
        "Oftentimes, however, we’re interested in a specific category-- for example, how well the classifier did with respect to the “poetry” category in particular.\n",
        "\n",
        "So if we’re considering a single category, we need a few more numbers: first, how many reviews belonging to the poetry category there are in our test dataset; second, how many times we’ve guessed (either correctly or incorrectly) that a test review belongs to the poetry category; and third, how many times we have guessed *correctly* that a test review belongs to the “poetry\" category.\n",
        "\n",
        "If we have this information, we can use it to calculate a couple of standard measures of classification efficiency: **precision** and **recall**.\n",
        "\n",
        "**Precision** tells us how often we correctly guessed that a review was in the poetry category.\n",
        "\n",
        "**Recall** tells us what *proportion* of the actual poetry reviews we caught.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6YmW7WZMhh3"
      },
      "source": [
        "Conveniently, we can use scikit-learn's `classification_report` function to generate our precision and recall scores (and another score, called F1, which is a compound score that takes both precision and recall into account).\n",
        "\n",
        "Importantly, we can see that our average scores are above random performance (we have 8 classes, so random performance would be ~0.2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeJd8ogKjpg0"
      },
      "source": [
        "print(classification_report(test_labels, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uK1l_gzUr9S"
      },
      "source": [
        "### **Finally, BERT!**\n",
        "\n",
        "But can we do better? This is where BERT comes in.\n",
        "\n",
        "Whereas with the last lab, we used BERT for what was, essentially, feature extraction: we used the pre-trained model to obtain contextual embeddings for each word in our poetry corpus, we never updated the weights of the model during this process.\n",
        "\n",
        "This time around, we're going to add a task-specific layer (or \"head\") to our pretrained model. And because we're adding this new layer, and because of how LLMs use backpropogation in addition to prediction in order to update the model's weights, we're going to end up updating the weights of the entire model as we fine-tune it for this particular classification task.\n",
        "\n",
        "**Note to LK:** explain instruct-tuned models; also discuss computational costs and shift to prompt-based classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aow3FPpppZVE"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Encode data for BERT**\n",
        "\n",
        "We start *almost* the same way as before, but in addition to the encoding the reviews themselves, we also need to pass in the label for each review. We'll get to the labels in a minute, but this first step should be familiar.\n",
        "\n",
        "The reviews need to be truncated if they're more than 512 tokens or padded if they're fewer than 512 tokens. The tokens, or words in the texts, also need to be separated into \"word pieces\" and matched to their embedding vectors.\n",
        "\n",
        "We also need to add special tokens to help BERT:\n",
        "\n",
        "| BERT special token | Explanation |\n",
        "| --------------| ---------|\n",
        "| [CLS] | Start token of every document. |\n",
        "| [SEP] | Separator between each sentence |\n",
        "| [PAD] | Padding at the end of the document as many times as necessary, up to 512 tokens |\n",
        "|  &#35;&#35; | Start of a \"word piece\" |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRs0dEIoUZtV"
      },
      "source": [
        "Here we will load `DistilBertTokenizerFast` from the HuggingFace library, which will do all the work of encoding the texts for us. The `tokenizer()` will break word tokens into word pieces, truncate to 512 tokens, and add padding and special BERT tokens.\n",
        "\n",
        "Note that if you are dealing with documents that are significantly longer than 512 tokens, you'll likely want to divide your documents into chunks of 512 tokens rather than truncate them. Because these reviews are mostly undet 512 tokens though, we're just going to chop off what doesn't fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BEvRqpGVMUD"
      },
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name) # The model_name needs to match our pre-trained model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj8X7B30UvSj"
      },
      "source": [
        "## Add in labels\n",
        "\n",
        "The labels (here, genres) need to be turned into integers rather than strings.\n",
        "\n",
        "Here we will create a map of our labels (genres) to integer keys. We take the unique labels, and then we make a dictionary that associates each label/tag with an integer.\n",
        "\n",
        "**Note:** HuggingFace documentation sometimes refers to \"labels\" as \"tags\" but these are the same thing. We use \"labels\" throughout this notebook for clarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSuo8gktjsVR"
      },
      "source": [
        "unique_labels = set(label for label in train_labels)\n",
        "label2id = {label: id for id, label in enumerate(unique_labels)}\n",
        "id2label = {id: label for label, id in label2id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_iAWMtBpfhj"
      },
      "source": [
        "label2id.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vle8EgkelwRa"
      },
      "source": [
        "id2label.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EraNFBC8VnPu"
      },
      "source": [
        "Now let's encode our texts and labels!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDuGq_n4pgZX"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "test_encodings  = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "train_labels_encoded = [label2id[y] for y in train_labels]\n",
        "test_labels_encoded  = [label2id[y] for y in test_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X1sYEGsWDDh"
      },
      "source": [
        "**Examine a Goodreads review in the training set after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A89SN_ppiUP"
      },
      "source": [
        "' '.join(train_encodings[345].tokens[0:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hvOn9RGWUMm"
      },
      "source": [
        "**Examine a Goodreads review in the test set after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OafZQFKSwG9E"
      },
      "source": [
        "' '.join(test_encodings[0].tokens[0:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jmt15FvW8FR"
      },
      "source": [
        "**Examine the training labels after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciemdVYwwMNz"
      },
      "source": [
        "set(train_labels_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK1Ngb0wXBz9"
      },
      "source": [
        "**Examine the test labels after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TowwulYQwOff"
      },
      "source": [
        "set(test_labels_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChcEv01TXI7v"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Make a custom Torch dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxWcyj0LXVtY"
      },
      "source": [
        "Here we combine the encoded labels and texts into dataset objects. We use the custom Torch `MyDataSet` class to make a `train_dataset` object from  the `train_encodings` and `train_labels_encoded`. We also make a `test_dataset` object from `test_encodings`, and `test_labels_encoded`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4VCU-nepnqF"
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyCFH3XEi4Ng"
      },
      "source": [
        "train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
        "test_dataset = MyDataset(test_encodings, test_labels_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSoXOmDYYyAK"
      },
      "source": [
        "**Examine a Goodreads review in the Torch `training_dataset` after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbJerEgC1Qpc"
      },
      "source": [
        "' '.join(train_dataset.encodings[0].tokens[0:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examine the label for that review:**"
      ],
      "metadata": {
        "id": "-9kOcwdn2HnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.labels[0]"
      ],
      "metadata": {
        "id": "a-_6O4Jb2GdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use our value-to-key function to print out its label\n",
        "print(id2label[train_dataset.labels[0]])\n"
      ],
      "metadata": {
        "id": "TNUFRDUs2tLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq1M2Et4Y3LB"
      },
      "source": [
        "**Examine a Goodreads review in the Torch `test_dataset` after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z65jnjVJ1aVB"
      },
      "source": [
        "' '.join(test_dataset.encodings[1].tokens[0:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the label for the corresponding review in the test dataset\n",
        "print(id2label[test_dataset.labels[1]])"
      ],
      "metadata": {
        "id": "QeJ7Yvh521Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkVgFcbCqKSu"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Load pre-trained BERT model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2pSuFUVaDhP"
      },
      "source": [
        "Now we load the pre-trained DistilBERT model and send it to CUDA.\n",
        "\n",
        "Our fine-tuning will sit on top of these layers.\n",
        "\n",
        "**Note:** If you decide to repeat fine-tuning after already running the following cells, make sure that you re-run this cell to re-load the original pre-trained model before fine-tuning again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7k75REXp7UJ"
      },
      "source": [
        "# The model_name needs to match the name used for the tokenizer above.\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label)).to(device_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRQZuqcAqQNI"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Set the BERT fine-tuning parameters**\n",
        "\n",
        "These are the arguments we'll set in the HuggingFace TrainingArguments objects, which we'll then pass to the HuggingFace Trainer object. There are many more possible arguments, but here we highlight the basics and some common gotchas.\n",
        "\n",
        "When training your own model, you should search over these parameters to find the best settings for your particular dataset. You should use a held-out set of validation data for this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYOaH9AhbCD_"
      },
      "source": [
        "| Parameter | Explanation |\n",
        "|-----------| ------------|\n",
        "| num_train_epochs | total number of training epochs (how many times to pass through the entire dataset; too much can cause overfitting) |\n",
        "| per_device_train_batch_size | batch size per device during training |\n",
        "| per_device_eval_batch_size |  batch size for evaluation |\n",
        "|  warmup_steps |  number of warmup steps for learning rate scheduler (set lower because of small dataset size) |\n",
        "| weight_decay | strength of weight decay (reduces size of weights, like regularization) |\n",
        "| output_dir | output directory for the fine-tuned model and configuration files |\n",
        "| logging_dir | directory for storing logs |\n",
        "| logging_steps | how often to print logging output (so that we can stop training early if the loss isn't going down) |\n",
        "| evaluation_strategy | evaluate while training so that we can see the accuracy going up |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3idCswBVg6v_"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    learning_rate=5e-5,              # initial learning rate for Adam optimizer\n",
        "    warmup_steps=100,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    output_dir='./results',          # output directory\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
        "    eval_strategy='steps',           # evaluate during fine-tuning so that we can see progress\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pb3xtidn-HJ"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Fine-tune the BERT model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_SN_oGLV8Vw"
      },
      "source": [
        "First, we define a custom evaluation function that returns the accuracy. You could modify this function to return precision, recall, F1, and/or other metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNIt7fcnqUCp"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9xl5QLmWsAw"
      },
      "source": [
        "Then we create a HuggingFace `Trainer` object using the `TrainingArguments` object that we created above. We also send our `compute_metrics` function to the `Trainer` object, along with our test and train datasets.\n",
        "\n",
        "**Note:** This is what we've been aiming for this whole time! All the work of tokenizing, creating datasets, and setting the training arguments was for this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgc8FS50qV0_"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo5QVLYjXGCN"
      },
      "source": [
        "Time to finally fine-tune!\n",
        "\n",
        "Be patient; if you've set everything in Colab to use GPUs, then it should only take a minute or two to run, but if you're running on CPU, it can take hours.\n",
        "\n",
        "After every 10 steps (as we specified in the TrainingArguments object), the trainer will output the current state of the model, including the training loss, validation (\"test\") loss, and accuracy (from our `compute_metrics` function).\n",
        "\n",
        "You should see the loss going down and the accuracy going up. If instead they are staying the same or oscillating, you probably need to change the fine-tuning parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W64JwriVqcmk"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXeIZ_LFqeps"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Save fine-tuned model**\n",
        "\n",
        "The following cell will save the model and its configuration files to a directory in Colab. To preserve this model for future use, you should download the model to your computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxkDWDfvqeAo"
      },
      "source": [
        "trainer.save_model(cached_model_directory_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or save to drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/My Drive/\" + cached_model_directory_name\n",
        "\n",
        "trainer.save_model(path)"
      ],
      "metadata": {
        "id": "2h4En955G3C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epiLftYkZrzc"
      },
      "source": [
        "(Optional) If you've already fine-tuned and saved the model, you can reload it using some of the following code. You don't have to run fine-tuning every time you want to evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A54QySLrO5I"
      },
      "source": [
        "# load local file\n",
        "# trainer = DistilBertForSequenceClassification.from_pretrained(cached_model_directory_name)\n",
        "\n",
        "# load file from your gdrive\n",
        "# trainer = DistilBertForSequenceClassification.from_pretrained(path)\n",
        "\n",
        "# load zipped file from Dr. Klein's google drive\n",
        "#gdown.download('https://drive.google.com/uc?export=download&id=1ptxZSBE_kQNM5dDf76p01XFoKcMdf2MV', quiet=False)\n",
        "\n",
        "# unzip it\n",
        "#!unzip distilbert-reviews-genres.zip\n",
        "\n",
        "# load local version as above\n",
        "#trainer = DistilBertForSequenceClassification.from_pretrained(\"distilbert-reviews-genres\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpzV4hFsLmZ6"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Evaluate fine-tuned model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IvfhrBtYYcz"
      },
      "source": [
        "The following function of the `Trainer` object will run the built-in evaluation, including our `compute_metrics` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dshtTH0WLtM1"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILJGLcCjYhPt"
      },
      "source": [
        "But we might want to do more fine-grained analysis of the model, so we extract the predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_E8oVjeLuv2"
      },
      "source": [
        "predicted_results = trainer.predict(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUYGfzczOuJE"
      },
      "source": [
        "predicted_results.predictions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqUTa5irLyN8"
      },
      "source": [
        "predicted_labels = predicted_results.predictions.argmax(-1) # Get the highest probability prediction\n",
        "predicted_labels = predicted_labels.flatten().tolist()      # Flatten the predictions into a 1D list\n",
        "predicted_labels = [id2label[l] for l in predicted_labels]  # Convert from integers back to strings for readability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2jtqnJbPMpu"
      },
      "source": [
        "len(predicted_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVcMU45fLzli"
      },
      "source": [
        "print(classification_report(test_labels,\n",
        "                            predicted_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddk-iKTQF-K3"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Pull out correct and incorrect classifications for examination**\n",
        "\n",
        "Let's use our predicted labels for some analysis!\n",
        "\n",
        "Now that we've fine-tuned and pulled out our predicted labels, the BERT part of this tutorial is done. You can now use the predicted labels in the same way you would use any set of predicted labels from any classification model. We'll show some examples here.\n",
        "\n",
        "First, let's print out some example predictions that were correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7KrvGuZkDAV"
      },
      "source": [
        "for _true_label, _predicted_label, _text in random.sample(list(zip(test_labels, predicted_labels, test_texts)), 20):\n",
        "  if _true_label == _predicted_label:\n",
        "    print('LABEL:', _true_label)\n",
        "    print('REVIEW TEXT:', _text[:100], '...')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW30Z6ynkDPI"
      },
      "source": [
        "Now let's print out some misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmx1RSKDkIpG"
      },
      "source": [
        "for _true_label, _predicted_label, _text in random.sample(list(zip(test_labels, predicted_labels, test_texts)), 20):\n",
        "  if _true_label != _predicted_label:\n",
        "    print('TRUE LABEL:', _true_label)\n",
        "    print('PREDICTED LABEL:', _predicted_label)\n",
        "    print('REVIEW TEXT:', _text[:100], '...')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MZqyFrckJBB"
      },
      "source": [
        "Finally, let's create some heatmaps to examine misclassification patterns. We could use these patterns to think about similarities and differences between genres, according to book reviewers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8yJ3-Z7hLXf"
      },
      "source": [
        "genre_classifications_dict = defaultdict(int)\n",
        "for _true_label, _predicted_label in zip(test_labels, predicted_labels):\n",
        "  genre_classifications_dict[(_true_label, _predicted_label)] += 1\n",
        "\n",
        "dicts_to_plot = []\n",
        "for (_true_genre, _predicted_genre), _count in genre_classifications_dict.items():\n",
        "  dicts_to_plot.append({'True Genre': _true_genre,\n",
        "                        'Predicted Genre': _predicted_genre,\n",
        "                        'Number of Classifications': _count})\n",
        "\n",
        "df_to_plot = pd.DataFrame(dicts_to_plot)\n",
        "df_wide = df_to_plot.pivot_table(index='True Genre',\n",
        "                                 columns='Predicted Genre',\n",
        "                                 values='Number of Classifications')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSAgS6tvivvz"
      },
      "source": [
        "plt.figure(figsize=(9,7))\n",
        "sns.set(style='ticks', font_scale=1.2)\n",
        "sns.heatmap(df_wide, linewidths=1, cmap='Purples')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiukJdYRqyjm"
      },
      "source": [
        "Looks good! We can see that overall, our model is assigning the correct labels for each genre.\n",
        "\n",
        "Now, let's remove the diagonal from the plot to highlight the misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7GJQfYxi3ET"
      },
      "source": [
        "genre_classifications_dict = defaultdict(int)\n",
        "for _true_label, _predicted_label in zip(test_labels, predicted_labels):\n",
        "  if _true_label != _predicted_label: # Remove the diagonal to highlight misclassifications\n",
        "    genre_classifications_dict[(_true_label, _predicted_label)] += 1\n",
        "\n",
        "dicts_to_plot = []\n",
        "for (_true_genre, _predicted_genre), _count in genre_classifications_dict.items():\n",
        "  dicts_to_plot.append({'True Genre': _true_genre,\n",
        "                        'Predicted Genre': _predicted_genre,\n",
        "                        'Number of Classifications': _count})\n",
        "\n",
        "df_to_plot = pd.DataFrame(dicts_to_plot)\n",
        "df_wide = df_to_plot.pivot_table(index='True Genre',\n",
        "                                 columns='Predicted Genre',\n",
        "                                 values='Number of Classifications')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oM09JGrjX_y"
      },
      "source": [
        "plt.figure(figsize=(9,7))\n",
        "sns.set(style='ticks', font_scale=1.2)\n",
        "sns.heatmap(df_wide, linewidths=1, cmap='Purples')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w99vVZCVl8g0"
      },
      "source": [
        "There's much more you can do with your own dataset and labels! Classification can be used to apply a small set of labels across a big dataset; to explore misclassifications to better understand users; and much more!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Adapted from a notebook created by the [BERT for Humanists](https://melaniewalsh.github.io/BERT-for-Humanists/) Team*\n",
        "<br></br>\n",
        "\n"
      ],
      "metadata": {
        "id": "XjkvF3Fhnf2m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEnxI21djZXT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}