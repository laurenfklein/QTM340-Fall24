{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2N1ZLUCMyQa"
      },
      "source": [
        "# LR Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thkyRj39MyQd"
      },
      "source": [
        "Classification, a task popular in machine learning, determines whether and how a model can distinguish between sets of text.\n",
        "\n",
        "It works like this. Everyone with email relies on classification to separate spam from legitimate emails. Email providers train classification models to recognize the difference by giving them emails they have labeled “spam” and “not spam.” They then ask the model to learn the features that most reliably distinguish the two types, which could include a preponderance of all caps or phrases like “free money” or “get paid.” They test the model by giving it unlabeled emails and asking it to classify them. If the model can do it accurately a high percentage of the time, that’s a good spam filter.\n",
        "\n",
        "We can take the underlying idea and apply it to many experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JwWzSwnMyQe"
      },
      "source": [
        "## Today's experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_2URE2jMyQe"
      },
      "source": [
        "We are going to use a corpus of obituaries from _The New York Times_ in order to test whether our model can learn to distinguish between obituaries about men and women."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64iAn2sLMyQf"
      },
      "source": [
        "## Imports\n",
        "\n",
        "As always, we begin with some imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSJJN37eMyQf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from pandas import DataFrame\n",
        "from pandas import Series, DataFrame\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.stats import pearsonr, norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDKkosNuMyQg"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfXOjwXyMyQh"
      },
      "source": [
        "For this notebook, we'll be using a dataset of _New York Times_ obituaries. Let's import the gdown library so that we can load in the dataset and unzip it:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading large files from Google Drive\n",
        "import gdown\n",
        "\n",
        "# Download the zip file\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1G0Aeg8dzZGPOCNFZ77U-s9CfEnR8efbB', quiet=False)"
      ],
      "metadata": {
        "id": "1oON3SrpOTzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip it\n",
        "!unzip NYT-Obituaries.zip"
      ],
      "metadata": {
        "id": "mY_g1gAyO6zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcOv-xMZMyQh"
      },
      "outputs": [],
      "source": [
        "# collect filepaths as files\n",
        "directory = \"./NYT-Obituaries/\"\n",
        "files = glob.glob(f\"{directory}/*.txt\")\n",
        "\n",
        "len(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kK-o6NoMyQi"
      },
      "outputs": [],
      "source": [
        "# and collect obit titles, which are also the final section of the filepaths\n",
        "obit_titles = [Path(file).stem for file in files]\n",
        "obit_titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLJ2Uk2bMyQi"
      },
      "source": [
        "## Create document-term matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM2H7u6oMyQj"
      },
      "source": [
        "### Initiate CountVectorizer as vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiK6x4y9MyQj"
      },
      "source": [
        "Remember document-term matrices, aka doc-term matrices, aka dtms? We learned about them in our tf-idf notebooks. Our classifier uses a dtm as its input. We build it with scikit-learn's CountVectorizer, which we already imported up above.\n",
        "\n",
        "When we load our vectorizer, we include an argument to encode as utf-8 and we load our stopwords. In this case, we're using a custom stopwords list rather than the default sklearn one. You may end up using a custom stopwords list in your final projects.\n",
        "\n",
        "In addition, we can set the minimum number of times a word must appear in the corpus for it to be included in the dtm. In this case, I've set it at 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb5ixhCAMyQj"
      },
      "outputs": [],
      "source": [
        "# another sklearn library to help load stopwords\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "# Download the stopwords file\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1BQ8zVSiG_WKpXNB81Y1P9yyi2L43UeJD', quiet=False)\n",
        "\n",
        "# open it\n",
        "text_file = open('./jockers_stopwords.txt')\n",
        "jockers_words = text_file.read().split()\n",
        "new_stopwords = list(text.ENGLISH_STOP_WORDS.union(jockers_words))\n",
        "\n",
        "# create dtm\n",
        "corpus_path = './NYT-Obituaries/'\n",
        "vectorizer = CountVectorizer(input='filename',\n",
        "                             encoding='utf8',\n",
        "                             stop_words = new_stopwords,\n",
        "                             min_df=20, dtype='float64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hpg5fHzMyQk"
      },
      "source": [
        "### Make list of filepaths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HpIit-FMyQk"
      },
      "source": [
        "If you recall, CountVectorizer builds a dtm from a list of filepaths. So we will provide that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWrj3EY4MyQk"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "for title in obit_titles:\n",
        "    filename = title + \".txt\"\n",
        "    corpus.append(corpus_path + filename)\n",
        "dtm = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvHgq_OYMyQk"
      },
      "source": [
        "### Get feature names and set as column titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebq6_IHkMyQl"
      },
      "source": [
        "The columns store word counts. We want to name the columns with the words stored in each, and to transform the dtm into a pandas dataframe, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S8nW0-oMyQl"
      },
      "outputs": [],
      "source": [
        "features = vectorizer.get_feature_names_out()\n",
        "dtm_array = dtm.toarray()\n",
        "dtm_df = DataFrame(dtm_array, columns=features)\n",
        "print('df shape is: ' + str(df.shape))\n",
        "\n",
        "dtm_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zaBoaGcMyQl"
      },
      "source": [
        "Our dataframee has 378 rows, one for each document, or obituary, and 2985 columns, one for each word that's not in stopwords and appears at least 20 times in the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv8svLFEMyQl"
      },
      "source": [
        "### Pandas interlude ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jObwxn2UMyQl"
      },
      "source": [
        "I re-found [this](https://twitter.com/mmitchell_ai/status/1454931443386228751) on Twitter the other night, posted by Dr. Margaret Mitchell, the former co-lead of Google's EthicalAI group and now Big Deal at HuggingFace (the folx behind the transformer libraries we'll be using next week):\n",
        "\n",
        "<img src=\"http://lklein.com/wp-content/uploads/2021/11/Screen-Shot-2021-11-01-at-10.14.24-AM.png\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lhoTQ2zMyQm"
      },
      "source": [
        "In any case, now it's time to import our metadata:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo8gFjIMMyQm"
      },
      "source": [
        "## Import metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWOEvp-zMyQm"
      },
      "outputs": [],
      "source": [
        "gdown.download('https://drive.google.com/uc?export=download&id=1Pca0n-vWTy_FcF0oKWt9iHwxuBrDnxfd', quiet=False)\n",
        "\n",
        "metadata = pd.read_csv(\"./NYT-Obituaries.csv\", encoding = 'utf-8')\n",
        "metadata = metadata.rename(columns={'title': 'obit_title'})\n",
        "metadata = metadata[[\"obit_title\", \"gender\", \"date\"]]\n",
        "metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYtqyBnlMyQm"
      },
      "source": [
        "Our metadata is stored as a pandas dataframe with a row for each obituary and three columns: title, gender, and year.\n",
        "\n",
        "Let's now concatinate the dtm to it so that everything is in one place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF3xLu-sMyQm"
      },
      "source": [
        "## Concatenate metadata and doc-term dataframe\n",
        "\n",
        "We'll use the pandas `concat` methods, specifying that the data should be concatinated as additional columns (that's the `axis = 1` parameter. (The default would be `0` to concatinate as additional rows.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRkWETjgMyQm"
      },
      "outputs": [],
      "source": [
        "df_concat = pd.concat([metadata, dtm_df], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlligcekMyQm"
      },
      "outputs": [],
      "source": [
        "df_concat.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjdTSL38MyQm"
      },
      "source": [
        "## Equalize numbers of men and women"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5K9dMgrMyQn"
      },
      "source": [
        "We want our dataframe to have equal numbers of men and women. How many women are there? Women are counted as 1 and men as 0, so if we sum the gender column, we'll have the number of women:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrzPwbpaMyQn"
      },
      "outputs": [],
      "source": [
        "metadata['gender'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO3r8chHMyQn"
      },
      "source": [
        "Then we separate men and women into two dataframes and take a random sample of 93 obituaries about men. Then we concatenate the sampled men dataframe with the original women dataframe and reset the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzU5v09CMyQn"
      },
      "outputs": [],
      "source": [
        "# separate by gender\n",
        "df_men = df_concat[df_concat['gender'] == 0]\n",
        "df_women = df_concat[df_concat['gender'] == 1]\n",
        "\n",
        "# sample 93 of the men\n",
        "df_93_men = df_men.sample(n=93)\n",
        "\n",
        "# concatenate the 93 men and 93 women\n",
        "df_final = pd.concat([df_93_men, df_women])\n",
        "\n",
        "# reset the index\n",
        "df_final = df_final.reset_index()\n",
        "df_final = df_final.drop(columns=\"index\")\n",
        "\n",
        "df_final\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jErtMyTgMyQo"
      },
      "source": [
        "### Match metadata and data dataframes with subset of df_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teb5WqNHMyQo"
      },
      "source": [
        "We'll continue to use the isolated metadata associated with our subsampled dataset, as well as the corresponding dtm, so we need to ensure they match our subsetted df_final dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRgiezEIMyQo"
      },
      "outputs": [],
      "source": [
        "metadata = df_final[[\"obit_title\", \"gender\", \"date\"]]\n",
        "metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrCMOuC3MyQo"
      },
      "outputs": [],
      "source": [
        "dtm_df = df_final.loc[:,'000':]\n",
        "\n",
        "dtm_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le-nksyzMyQo"
      },
      "source": [
        "## Let's run our classifier!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF3jMAmkMyQo"
      },
      "source": [
        "Once we have a dataframe with metadata and feature counts we're ready to run our classifier!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0y_gS7JMyQo"
      },
      "source": [
        "### We add columns to hold the probabilities and predicted class to our metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LQhLr2gMyQo"
      },
      "source": [
        "As we run the model, we are going to store its output with our metadata. This will allow us to easily examine the model's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g__-yBjbMyQo"
      },
      "outputs": [],
      "source": [
        "metadata.loc[:, 'PROBS'] = np.nan\n",
        "metadata.loc[:, 'PREDICTED'] = np.nan\n",
        "\n",
        "metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_IHinhIMyQp"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSp_jqk9MyQp"
      },
      "source": [
        "We will use scikit-learn's `LogisticRegression` model. There are many other options for classifier models. Some are better for some tasks, other for others. LogisticRegression is standard for classifying literature. We set the penalty as L1 (Lasso / least absolute deviations) and the 'C' value as 1.0.\n",
        "\n",
        "If you decide to specialize in classification, you can explore further the implications of these regularization parameters. If you're curious now, [here is a nice intuitive primer](https://medium.com/analytics-vidhya/l1-vs-l2-regularization-which-is-better-d01068e6658c) on L1 vs the other main option, L2 (Ridge Regression). (In sk-learn, there is also an option in the middle, the Elastic-Net, which leads to an amount of sparsity between that of L1 and L2.)\n",
        "\n",
        "As far as C, large values of C (like ours) give more freedom to the model. Conversely, smaller values of C (e.g. .1 or .01) constrain the model more.\n",
        "\n",
        "`class_weight` adjusts the weights of classes inversely proportional to their frequencies in the data. This is particularly useful for imbalanced datasets, where one class may dominate. The model will give more weight to the minority class to handle imbalances.\n",
        "\n",
        "The `solver` is used to optimize the model. 'liblinear' is a good choice when using L1 regularization, and it's suitable for smaller datasets as it uses coordinate descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrQ3Gan5MyQp"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(penalty = 'l1',\n",
        "                           C = 1.0,\n",
        "                           class_weight='balanced',\n",
        "                           solver='liblinear')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the model!\n",
        "\n",
        "We run the model in the following for-loop.\n",
        "\n",
        "Classification models need classes: they need the texts grouped into different sets. Our metadata has built-in classes: gender. Men are stored as 0; women as 1. We could, if we wanted, create a new 0/1 class based on something else.\n",
        "\n",
        "Each iteration trains on all the obits except one, then predicts which class the excluded obit belongs to. This is called leave-one-out classification. It's helpful when you're working with a small dataset. There are other ways of dividing training and testing sets, including the 80/20 split we used lass class with our book reviews genres classifier."
      ],
      "metadata": {
        "id": "YTOA521zEzml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = None  # initialize weights variable for future use\n",
        "\n",
        "for index, row in dtm_df.iterrows():\n",
        "  print(\"Obit #\" + str(index)) # keep track of where we are in the corpus\n",
        "\n",
        "  X_train = dtm_df.drop(index)    # create training set X-train that includes all but current row (index)\n",
        "  y_train = metadata['gender'].drop(index)  # create labels for training set (y_train) that includes all labels but current row (index)\n",
        "  X_test = pd.DataFrame([row])    # create test set of just the current row\n",
        "  y_test = metadata['gender'][index] # extracts actual gender label of current row for future comparison\n",
        "\n",
        "  model.fit(X_train, y_train) # fit the model using the training data (X_train) and associated labels (y_train)\n",
        "  y_pred = model.predict(X_test)[0] # create predictions on the test set, which is the current row (X_test)\n",
        "\n",
        "  # print out some status messages\n",
        "  print(f\"{metadata['obit_title'][index]}, actual gender: {y_test}, predicted gender: {y_pred}\")\n",
        "\n",
        "  metadata.loc[index, 'PREDICTED'] = y_pred # store predicted gender\n",
        "  metadata.loc[index, 'PROBS'] = model.predict_proba(X_test)[0][1] # caculate probability of predicted class (gender)\n",
        "\n",
        "  # keep track of accumulted feature weights over time\n",
        "  # later, we will average these to get the average feature weights\n",
        "  if weights is None:\n",
        "    weights = model.coef_[0]\n",
        "  else:\n",
        "    weights += model.coef_[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "-QjrTDcUBS4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ybib5lCnPppE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at our results as stored in the `metadata` dataframe."
      ],
      "metadata": {
        "id": "jlCs7ireIpaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata"
      ],
      "metadata": {
        "id": "Fp-tkBt9JCzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check accuracy\n",
        "\n",
        "It's still a little hard to get an overall sense of things, so let's calculate overall accuracy."
      ],
      "metadata": {
        "id": "B2eILZvbJGC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up brackets\n",
        "metadata = metadata.replace([0], 0)\n",
        "metadata = metadata.replace([1], 1)\n",
        "\n",
        "# add result column\n",
        "sum_column = metadata['gender'] - metadata['PREDICTED']\n",
        "metadata['RESULT'] = sum_column\n",
        "\n",
        "# check accuracy\n",
        "metadata_correct = metadata[metadata['RESULT'] == 0]\n",
        "accuracy = len(metadata_correct) / len(metadata)\n",
        "print(\"Model accuracy: \" + str(accuracy))"
      ],
      "metadata": {
        "id": "zuvmLxicIupD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hm... not a very good classifier! At random, the model should guess correctly 50% of the times.\n",
        "\n",
        "## Plot a confusion matrix\n",
        "\n",
        "Let's just take a look at a confusion matrix to see if anything interesting is going on before we move on. Here's some code to do that."
      ],
      "metadata": {
        "id": "nipEW-ukJR_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "actual = metadata['gender'].array\n",
        "predicted = metadata['PREDICTED'].array\n",
        "\n",
        "cm = confusion_matrix(actual, predicted)\n",
        "\n",
        "print(cm)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['man','woman'])\n",
        "\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E_WEEWJcJZDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate precision and recall\n",
        "\n",
        "We're going to move on soon, but so you have it, here's how we calculate precision and recall in this context:"
      ],
      "metadata": {
        "id": "BY-Ha1CCJ8S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "y_true = metadata['gender']\n",
        "y_pred = metadata['PREDICTED']\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n"
      ],
      "metadata": {
        "id": "WYDY8gccKs_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examining features\n",
        "\n",
        "Sometimes it's not as much the overall accuracy of the classifier that's interesting as much as the features themselves. Let's explore which ones most help the model make its classification decision.\n",
        "\n",
        "In this case, the features tell us which words are most likely to tip off the model that a given obituary is about a man or that another is about a woman.\n",
        "\n",
        "### Calculating p-values\n",
        "\n",
        "This part should be familiar to many of you. We'll define a Z-test function and use it to calculate p-values. We'll use these to determine the statistical significance of individual features.\n",
        "\n",
        "(Our null hypothesis here, as in all z-tests, is a normal distribution.)\n",
        "\n",
        "Note: As many of you might know, p-values have come under scrutiny in recent years as a measure of significance. The standard threshold for signficance (0.05) is arbitrary. Its meaning is debated. But its authority for a long time incentivized what came to be called p-hacking: the practice of manipulating ones work so it would pass the 0.05 threshold and count as significant. Very recently statisticians and other scholars have argued for abandoning the term \"statistical significance,\" arguing that it reduces the complexity of determining whether a given result is meaningful in context. tl;dr: don't put too much stock in significance; consider the holistic context of results"
      ],
      "metadata": {
        "id": "Fh6woG4vKxa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Ztest(vec1, vec2):\n",
        "    # calculate means, standard deviations, and lengths of the two data vectors\n",
        "    X1, X2 = np.mean(vec1), np.mean(vec2)\n",
        "    sd1, sd2 = np.std(vec1, ddof=1), np.std(vec2, ddof=1)\n",
        "    n1, n2 = len(vec1), len(vec2)\n",
        "\n",
        "    # calculate pooled standard error\n",
        "    pooledSE = np.sqrt(sd1**2/n1 + sd2**2/n2)\n",
        "\n",
        "    # compute Z statistic\n",
        "    z = (X1 - X2) / pooledSE\n",
        "\n",
        "    # compute p-value\n",
        "    pval = 2 * norm.sf(abs(z))\n",
        "\n",
        "    return z, pval"
      ],
      "metadata": {
        "id": "gugV_kB1MSOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use it to caculate p-values for each of the features using our `metadata` and `dtm_df` dataframes."
      ],
      "metadata": {
        "id": "9meNGiftNdKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_data = []  # create a list to store feature information\n",
        "\n",
        "# calculate average feature weights\n",
        "avg_weights = weights / len(dtm_df)\n",
        "\n",
        "for feature in dtm_df.columns:\n",
        "  # separate data for men and women based on the current feature.\n",
        "  men_feature_data = dtm_df.loc[metadata['gender'] == 0, feature]\n",
        "  women_feature_data = dtm_df.loc[metadata['gender'] == 1, feature]\n",
        "\n",
        "  # calculate z-test and p-value using the Ztest function.\n",
        "  z_score, p_value = Ztest(men_feature_data, women_feature_data)\n",
        "\n",
        "  # append feature data to the list\n",
        "  feature_data.append({\n",
        "    'feature': feature,\n",
        "    'p_value': p_value,\n",
        "    'avg_weight': avg_weights[dtm_df.columns.get_loc(feature)]\n",
        "  })\n",
        "\n",
        "# convert the list of dictionaries into a DataFrame\n",
        "feature_df = pd.DataFrame(feature_data)\n",
        "\n",
        "# sort by average weight then print\n",
        "feature_df = feature_df.sort_values('avg_weight', ascending = True)\n",
        "\n",
        "feature_df\n"
      ],
      "metadata": {
        "id": "bMVcap9SNGQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is more interesting! The negative LR weights tell the model that the obituary is likely about a woman. The positive LR weights tell the model the obituary is likely about a man. Let's take a slightly closer look."
      ],
      "metadata": {
        "id": "Wu7u2ZO8Rkx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIIHzCoCMyQs"
      },
      "outputs": [],
      "source": [
        "print(\"Words associated with women:\")\n",
        "feature_df.sort_values('avg_weight', ascending = True).head(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can you infer from these words about the coverage of famous women in their obituaries in the NYT?"
      ],
      "metadata": {
        "id": "v3TOC6lZSQQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Words associated with men:\")\n",
        "feature_df.sort_values('avg_weight', ascending = False).head(20)"
      ],
      "metadata": {
        "id": "674HL9boSFHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can you infer from these words about the coverage of famous men in their obituaries in the NYT?\n",
        "\n",
        "## Filtering by p-value\n",
        "\n",
        "Finally, we can filter further by setting a p-value threshold. For our purposes, we can set a high threshold, which to normalize we need to divide by the number of features\n",
        "\n",
        "Remember significance is a contested concept. What most matters is understanding the meaning of numbers in context. For us, any feature with a logistic regression weight gives us useful information, and p-values help us understand just how robust that feature is."
      ],
      "metadata": {
        "id": "NE3OSDJ8SWaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to account for the many features / mulitple hypothesis tests\n",
        "# sig_thresh = 50.00 / len(feature_df)\n",
        "\n",
        "sig_thresh = .05\n",
        "print(\"Significance threshold: \" + str(sig_thresh))\n",
        "\n",
        "# filter by p-values that pass that threshold\n",
        "out = feature_df[(feature_df['p_value'] <= sig_thresh)].sort_values('avg_weight', ascending = True)\n",
        "out = out[out['avg_weight'] != 0]\n",
        "outM = out[out['avg_weight'] >= 0]\n",
        "outW = out[out['avg_weight'] <= 0]\n",
        "\n",
        "# pass remaining features to list and print them out\n",
        "outM = outM['feature'].tolist()\n",
        "print(\"Here are significant words that distinguish men: \" + str(outM))\n",
        "\n",
        "outW = outW['feature'].tolist()\n",
        "print(\"Here are significant words that distinguish women: \" + str(outW))"
      ],
      "metadata": {
        "id": "H1y2wuBISK4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And that's it!"
      ],
      "metadata": {
        "id": "FyAmSU_qTmKM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKTJI_KCS-gm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}