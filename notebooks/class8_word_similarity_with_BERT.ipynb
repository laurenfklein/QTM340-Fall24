{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "quwSCsFDUywu"
      ],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucj6rr9Z7gt3"
      },
      "source": [
        "# (More) Word Similarity with BERT\n",
        "\n",
        "Since we were online last class, I want to review a few key components of the notebook from last class to make sure that we're all on the same page.\n",
        "\n",
        "We'll be jumping into last class's notebook halfway through, with a few things added to the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y73xWUkpU1AC"
      },
      "source": [
        "## **Import necessary Python libraries and modules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ9kmfrO3M_w"
      },
      "source": [
        "First we will import the DistilBertModel and DistilBertTokenizerFast from Hugging Face. We will also import a handful of other Python libraries and modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lcEPhcuV9f5"
      },
      "source": [
        "# For BERT\n",
        "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
        "\n",
        "# For data manipulation and analysis\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 200\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# For interactive data visualization\n",
        "import altair as alt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_C4hiuqvt5L"
      },
      "source": [
        "## **Load text dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4olw75x3vt5N"
      },
      "source": [
        "Same as last class, our dataset contains around ~30 thousand poems scraped from  http://public-domain-poetry.com/. This website hosts a curated collection of poems that have fallen out of copyright, which makes them easier for us to share on the web.\n",
        "You can find the data in Melanie Walsh's [GitHub repository](https://github.com/melaniewalsh/BERT-4-Humanists/blob/main/data/public-domain-poetry.csv).\n",
        "\n",
        "We don't have granular date information about when each poem was published, but we do know the birth dates of most of our authors, which we've used to loosely categorize the poems by time period. The poems in our data range from the Middle Ages to the 20th Century, but most come from the 19th Century. The data features both well-known authors — William Wordsworth, Emily Elizabeth Dickinson, Paul Laurence Dunbar, Walt Whitman, Shakespeare — as well as less well-known authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1tvz3gsvt5O"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/melaniewalsh/BERT-4-Humanists/main/data/public-domain-poetry.csv\"\n",
        "\n",
        "poetry_df = pd.read_csv(url, encoding='utf-8')\n",
        "\n",
        "# Show 5 random rows\n",
        "poetry_df.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFEhTJirgLL7"
      },
      "source": [
        "## **Sample text dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hebn8lYhOffh"
      },
      "source": [
        "Create a subsample, segmented by century.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdMUw_LWgjVF"
      },
      "source": [
        "# Filter the DataFrame for only a given time period, then randomly sample 1000 rows\n",
        "nineteenth_sample = poetry_df[poetry_df['period'] == '19th Century'].sample(1000)\n",
        "twentieth_sample = poetry_df[poetry_df['period'] == '20th Century'].sample(1000)\n",
        "eighteenth_sample = poetry_df[poetry_df['period'] == '18th Century'].sample(1000)\n",
        "sixteenth_sample = poetry_df[poetry_df['period'] == '16th-17th Centuries (Early Modern)'].sample(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXxtap-thFRk"
      },
      "source": [
        "# Merge these random samples into a new DataFrame\n",
        "poetry_df = pd.concat([sixteenth_sample, eighteenth_sample, twentieth_sample, nineteenth_sample])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z70pWivdvt5O"
      },
      "source": [
        "# make a list and check length\n",
        "poetry_texts = poetry_df['text'].tolist()\n",
        "\n",
        "len(poetry_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGi0nOaevt5P"
      },
      "source": [
        "Examine a poem in your dataset. Everyone's will be different!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl2UYJSkvt5P"
      },
      "source": [
        "print(poetry_texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTDNuhAqyMQC"
      },
      "source": [
        "## **Encode/tokenize text data for BERT**\n",
        "\n",
        "The encoding step is where I want us to start paying close attention.\n",
        "\n",
        "Here, we'll tokenize the poems with the `tokenizer()` from HuggingFace's `DistilBertTokenizerFast`. This is what the `tokenizer()` will do:\n",
        "\n",
        "1. Truncate the texts if they're more than 512 tokens or pad them if they're fewer than 512 tokens. If a word is not in BERT's vocabulary, it will be broken up into smaller \"word pieces,\" demarcated by a `##`.\n",
        "\n",
        "2. Add in special tokens to help BERT:\n",
        "    - [CLS] — Start token of every document\n",
        "    - [SEP] — Separator between each sentence\n",
        "    - [PAD] — Padding at the end of the document as many times as necessary, up to 512 tokens\n",
        "    - &#35;&#35; — Start of a \"word piece\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKgQkdodA-Jz"
      },
      "source": [
        "Here we will load `DistilBertTokenizerFast` from HuggingFace library, which will help us transform and encode the texts so they can be used with BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTa04n_-ZI-8"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDuGq_n4pgZX"
      },
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LbZPuonBIJl"
      },
      "source": [
        "The `tokenizer()` will break word tokens into word pieces, truncate to 512 tokens, and add padding and special BERT tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPoDCzIx_17F"
      },
      "source": [
        "tokenized_poems = tokenizer(poetry_texts, truncation=True, padding=True, return_tensors=\"pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MulyNi6jBMIK"
      },
      "source": [
        "Let's examine the first tokenized poem. We can see that the special BERT tokens have been inserted where necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A89SN_ppiUP"
      },
      "source": [
        "' '.join(tokenized_poems[0].tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Who can interpret this for us? What are the things in square brackets? Why are there so many hashtags?**"
      ],
      "metadata": {
        "id": "Kphe9zaeaADc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFXYdtl4aQ5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSAhcqQgaQ3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vGgzB9gjaQ0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkVgFcbCqKSu"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Load pre-trained BERT model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGVbuDJ0BXQ2"
      },
      "source": [
        "Now we will load a pre-trained BERT model, which we'll need to generate our contextual embeddings for each token. Here's where we also shift over to a GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1I4FthqzFjW"
      },
      "source": [
        "from transformers import DistilBertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7k75REXp7UJ"
      },
      "source": [
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if no GPU\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "Z-CFg4B1jtyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrau9RGwYic8"
      },
      "source": [
        "## **Get BERT embeddings for each document in a collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-vSpKnKBZWJ"
      },
      "source": [
        "This is the second step I want to move slowly through.\n",
        "\n",
        "The process is as follows: for each poem in our list `poetry_texts`, we will tokenize the poem, and we will extract the vocabulary word ID for each word/token in the poem (to use for later reference). Then we will run the tokenized poem through the BERT model, which does a couple of things beneath the surface:\n",
        "\n",
        "*Let's switch to [some slides](https://docs.google.com/presentation/d/1CoH6M1-_Hb_26b9c7uSM-pKCVZPN8jhS73UkNZYe2TA/edit#slide=id.p) for this...*\n",
        "\n",
        "The end result is that we extract the contextual embedding vectors for each word/token in the poem.\n",
        "\n",
        "We thus create two big lists for all the poems in our collection — `doc_word_ids` and `doc_word_vectors`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j58TvTCd_kxI"
      },
      "source": [
        "# List of vocabulary word IDs for all the words in each document (aka each poem)\n",
        "doc_word_ids = []\n",
        "# List of word vectors for all the words in each document (aka each poem)\n",
        "doc_word_vectors = []\n",
        "\n",
        "# Below we will slice our poem to ignore the first (0th) and last (-1) special BERT tokens\n",
        "start_of_words = 1\n",
        "end_of_words = -1\n",
        "\n",
        "# Below we will index the 0th or first document, which will be the only document, since we're analzying one poem at a time\n",
        "first_document = 0\n",
        "\n",
        "for i, poem in enumerate(poetry_texts):\n",
        "\n",
        "    # Here we tokenize each poem with the DistilBERT Tokenizer\n",
        "    inputs = tokenizer(poem, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Here we extract the vocabulary word ids for all the words in the poem (the first or 0th document, since we only have one document)\n",
        "    # We ignore the first and last special BERT tokens\n",
        "    # We also convert from a Pytorch tensor to a numpy array\n",
        "    doc_word_ids.append(inputs.input_ids[first_document].numpy()[start_of_words:end_of_words])\n",
        "\n",
        "    # Here we send the tokenized poems to the GPU\n",
        "    # The model is already on the GPU, but this poem isn't, so we send it to the GPU\n",
        " #   inputs.to(\"cuda\")\n",
        "    # Here we run the tokenized poem through the DistilBERT model\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # We take every element from the first or 0th document, from the 2nd to the 2nd to last position\n",
        "    # Grabbing the last layer is one way of getting token vectors. There are different ways to get vectors with different pros and cons\n",
        "    doc_word_vectors.append(outputs.last_hidden_state[first_document,start_of_words:end_of_words,:].detach().cpu().numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the first set of doc_word_ids\n",
        "doc_word_ids[0]"
      ],
      "metadata": {
        "id": "Pcb228MHmeyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and vectors\n",
        "doc_word_vectors[0]"
      ],
      "metadata": {
        "id": "jBX0o9fQmr9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf-9YAo814wM"
      },
      "source": [
        "Each element of these lists contains all the tokens/vectors for one document. But we want to concatenate them into two giant collections for ease of use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afOPAzVoczRW"
      },
      "source": [
        "all_word_ids = np.concatenate(doc_word_ids)\n",
        "all_word_vectors = np.concatenate(doc_word_vectors, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've got our word IDs and our vector IDs, we can move on."
      ],
      "metadata": {
        "id": "dRZ9W6hTk67q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F9IF6sqZuHK"
      },
      "source": [
        "## **Setting Up Cosine Similarity Calculations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQafcvCqt0Uf"
      },
      "source": [
        "Here's where we start to set up the cosine similarity calculations that we're going to do down below. Because cosine similarity measures the angle between vectors but ignores their length, we can speed this computation up by setting all the poem vectors to have length 1.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogECz92C4T09"
      },
      "source": [
        "# Calculating the length of each vector (Pythagorean theorem)\n",
        "row_norms = np.sqrt(np.sum(all_word_vectors ** 2, axis=1))\n",
        "# Dividing every vector by its length\n",
        "all_word_vectors /= row_norms[:,np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXG-ZNIlIOA9"
      },
      "source": [
        "Now that we've normalized our vectors so that each has a length of 1, we can calculate the cosine similiarity between one vector and any other (or set of others) by taking the dot product of the two vectors. Here's a function we can write to do that:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzxT2oC-3sDN"
      },
      "source": [
        "def get_nearest(query_vector, n=20):\n",
        "  cosines = all_word_vectors.dot(query_vector)\n",
        "  ordering = np.flip(np.argsort(cosines))\n",
        "\n",
        "  return ordering[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-ZnvlA0hEQJ"
      },
      "source": [
        "But, to make any results legible to humans, we need to be able to reattach these vectors to the words and contexts that they represent. Here are a bunch of functions that can help us do that:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBWkQbFLc6kt"
      },
      "source": [
        "## **More helpful functions**\n",
        "\n",
        "Just execute these next few cells so we have some helper functions to use.\n",
        "\n",
        "**Find all word positions in a collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUB1orgCek1D"
      },
      "source": [
        "This function uses the array `all_word_ids` to find all the places, or *positions*, in the collection where a word appears.\n",
        "\n",
        "We'll do this with the `tokenizer.vocab` attribute, finding the word's vocab ID in BERT and then checking to see where/how many times this ID occurs in `all_word_ids`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjjVsV8F1UvN"
      },
      "source": [
        "def get_word_positions(words):\n",
        "\n",
        "  \"\"\"This function accepts a list of words, rather than a single word\"\"\"\n",
        "\n",
        "  # Get word/vocabulary ID from BERT for each word\n",
        "  word_ids = [tokenizer.vocab[word] for word in words]\n",
        "\n",
        "  # Find all the positions where the words occur in the collection\n",
        "  word_positions = np.where(np.isin(all_word_ids, word_ids))[0]\n",
        "\n",
        "  return word_positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bB-69p-m4q7"
      },
      "source": [
        "For example, we can check to see all the places where the word \"bank\" appears in the collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZKWT3L32GZM"
      },
      "source": [
        "get_word_positions([\"bank\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx35pWc0dqIT"
      },
      "source": [
        "**Use get_word_positions to find word from word position**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9db1FKmAm-7p"
      },
      "source": [
        "The above function allow us to determine all the positions where the word \"bank\" appears in the collection. But it would be more helpful to us as humans to know the actual words that appear in context around it. To find these context words, we have to convert position IDs back into words, like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_0q-9Uodt__"
      },
      "source": [
        "# Here we create an array so that we can go backwards from numeric token IDs to words\n",
        "word_lookup = np.empty(tokenizer.vocab_size, dtype=\"O\")\n",
        "\n",
        "for word, index in tokenizer.vocab.items():\n",
        "    word_lookup[index] = word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Dt6HyVsidn"
      },
      "source": [
        "Now we can use `word_lookup` to find a word based on its position in the collection. Let's first see what the output of a basic lookup operation looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vSMuKJBoLuN"
      },
      "source": [
        "word_positions = get_word_positions([\"bank\"])\n",
        "\n",
        "for word_position in word_positions:\n",
        "  print(word_position, word_lookup[all_word_ids[word_position]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KwzuaBxpkwG"
      },
      "source": [
        "But we're not done yet. We can modify the above code to look for the 3 words that come before \"bank\" and the 3 words that come after it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ_zg3cNfJum"
      },
      "source": [
        "word_positions = get_word_positions([\"bank\"])\n",
        "\n",
        "for word_position in word_positions:\n",
        "\n",
        "  # Slice 3 words before \"bank\"\n",
        "  start_pos = word_position - 3\n",
        "  # Slice 3 words after \"bank\"\n",
        "  end_pos = word_position + 4\n",
        "\n",
        "  context_words = word_lookup[all_word_ids[start_pos:end_pos]]\n",
        "  # Join the words together\n",
        "  context_words = ' '.join(context_words)\n",
        "  print(word_position, context_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ssdFSBANkN"
      },
      "source": [
        "Now that we've tested out this approach, let's make some functions that will help us get the context words around a certain word position for whatever size window (certain number of words before and after) that we want.\n",
        "\n",
        "The first function `get_context()` will simply return the tokens without cleaning them, and the second function `get_context_clean()` will return the tokens in a more readable fashion. (Note the return of regex!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQDmbTABgQF9"
      },
      "source": [
        "def get_context(word_id, window_size=10):\n",
        "\n",
        "  \"\"\"Simply get the tokens that occur before and after word position\"\"\"\n",
        "\n",
        "  start_pos = max(0, word_id - window_size) # The token where we will start the context view\n",
        "  end_pos = min(word_id + window_size + 1, len(all_word_ids)) # The token where we will end the context view\n",
        "\n",
        "  # Make a list called tokens and use word_lookup to get the words for given token IDs from starting position up to the keyword\n",
        "  tokens = [word_lookup[word] for word in all_word_ids[start_pos:end_pos] ]\n",
        "\n",
        "  context_words = \" \".join(tokens)\n",
        "\n",
        "  return context_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lHshYA2-zxY"
      },
      "source": [
        "import re\n",
        "\n",
        "def get_context_clean(word_id, window_size=10):\n",
        "\n",
        "  \"\"\"Get the tokens that occur before and after word position AND make them more readable\"\"\"\n",
        "\n",
        "  keyword = word_lookup[all_word_ids[word_id]]\n",
        "  start_pos = max(0, word_id - window_size) # The token where we will start the context view\n",
        "  end_pos = min(word_id + window_size + 1, len(all_word_ids)) # The token where we will end the context view\n",
        "\n",
        "  # Make a list called tokens and use word_lookup to get the words for given token IDs from starting position up to the keyword\n",
        "  tokens = [word_lookup[word] for word in all_word_ids[start_pos:end_pos] ]\n",
        "\n",
        "  # Make wordpieces slightly more readable\n",
        "  # This is probably not the most efficient way to clean and correct for weird spacing\n",
        "  context_words = \" \".join(tokens)\n",
        "  context_words = re.sub(r'\\s+([##])', r'\\1', context_words)\n",
        "  context_words = re.sub(r'##', r'', context_words)\n",
        "  context_words = re.sub('\\s+\\'s', '\\'s', context_words)\n",
        "  context_words = re.sub('\\s+\\'d', '\\'d', context_words)\n",
        "  context_words = re.sub('\\s\\'er', '\\'er', context_words)\n",
        "  context_words = re.sub(r'\\s+([-,:?.!;])', r'\\1', context_words)\n",
        "  context_words = re.sub(r'([-\\'\"])\\s+', r'\\1', context_words)\n",
        "  context_words = re.sub('\\s+\\'s', '\\'s', context_words)\n",
        "  context_words = re.sub('\\s+\\'d', '\\'d', context_words)\n",
        "\n",
        "  # Bold the keyword by putting asterisks around it\n",
        "  if keyword in context_words:\n",
        "    context_words = re.sub(f\"\\\\b{keyword}\\\\b\", f\"**{keyword}**\", context_words)\n",
        "    context_words = re.sub(f\"\\\\b({keyword}[esdtrlying]+)\\\\b\", fr\"**\\1**\", context_words)\n",
        "\n",
        "  return context_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIL-7hVDk1ZE"
      },
      "source": [
        "## **Examining most similar vectors**\n",
        "\n",
        "At long last, we're ready to perform our similarity comparison. First we need to decide which particular instance of \"bank\" we want to examine. Let's print out some instances of \"bank\" in our poetry corpus to see if one seems interesting to us.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPIho4qIGGqp"
      },
      "source": [
        "# load some libraries for displaying text nicely\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def print_md(string):\n",
        "    display(Markdown(string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQP16q9Fgebk"
      },
      "source": [
        "# print out instances of \"bank\" in the corpus\n",
        "word_positions = get_word_positions(['bank'])\n",
        "\n",
        "for word_position in word_positions:\n",
        "\n",
        "  print_md(f\"<br> {word_position}: {get_context_clean(word_position)} <br>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlfj-soqJsqu"
      },
      "source": [
        "Now we can pick any of these keyword positions to compare to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us6ATXKhgz_k"
      },
      "source": [
        "keyword_position = 96827 # can be replaced with any of the above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hjubBfGm3J6"
      },
      "source": [
        "And at long last, display the most similar vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw_oyyOk5ual"
      },
      "source": [
        "contexts = [get_context_clean(token_id) for token_id in get_nearest(all_word_vectors[keyword_position,:])]\n",
        "\n",
        "for context in contexts:\n",
        "  print_md(context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOW3UJCU941G"
      },
      "source": [
        "## **Reduce word vectors via PCA and plot them**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xggu6BRkAgGu"
      },
      "source": [
        "We don't need to just *read* examples of similar usages of \"bank\" in the collection, we can also visualize them.\n",
        "\n",
        "But to do so, we need to perform some dimensionality reduction. We'll use PCA again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnaABMiLsTAQ"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "word_positions = get_word_positions([\"bank\"])\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "pca.fit(all_word_vectors[word_positions,:].T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UOMHTkGoHqn"
      },
      "source": [
        "Let's now make a list of all of the context views for our keyword so that we can associate them with their PCA score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV4UeeuKn-cK"
      },
      "source": [
        "word_positions = get_word_positions([\"bank\"])\n",
        "\n",
        "keyword_contexts = []\n",
        "keyword_contexts_tokens = []\n",
        "\n",
        "for position in word_positions:\n",
        "\n",
        "  keyword_contexts.append(get_context_clean(position))\n",
        "  keyword_contexts_tokens.append(get_context(position))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPWjXDxpevTe"
      },
      "source": [
        "Then, for convenience, we will put these PCA results into a Pandas DataFrame, which will use to generate an interactive plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUxuru27DrMm"
      },
      "source": [
        "df = pd.DataFrame({\"x\": pca.components_[0,:], \"y\": pca.components_[1,:],\n",
        "                   \"context\": keyword_contexts, \"tokens\": keyword_contexts_tokens})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quwSCsFDUywu"
      },
      "source": [
        "## **Match context with original text and metadata**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA3Cd2pgwDcY"
      },
      "source": [
        "It's helpful (and fun!) to know where each instance of a word actually comes from — which poem, which poet, which time period, which Public-Domain-Poetry.com web page. The easiest method we've found for matching a bit of context with its original poem and metdata is to 1) add a tokenized version of each poem to our original Pandas Dataframe 2) check to see if the context shows up in a poem 3) and if so, grab the original poem and metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmY6MMGIhWGJ"
      },
      "source": [
        "# Tokenize all the poems\n",
        "tokenized_poems = tokenizer(poetry_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Get a list of all the tokens for each poem\n",
        "all_tokenized_poems = []\n",
        "for i in range(len(tokenized_poems['input_ids'])):\n",
        "  all_tokenized_poems.append(' '.join(tokenized_poems[i].tokens))\n",
        "\n",
        "# Add them to the original DataFrame\n",
        "poetry_df['tokens'] = all_tokenized_poems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTeZ4WfCh_d_"
      },
      "source": [
        "poetry_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Uoqk2yQjQhH"
      },
      "source": [
        "def find_original_poem(rows):\n",
        "\n",
        "  \"\"\"This function checks to see whether the context tokens show up in the original poem,\n",
        "  and if so, returns metadata about the title, author, period, and URL for that poem\"\"\"\n",
        "\n",
        "  text = rows['tokens'].replace('**', '')\n",
        "  text = text[55:70]\n",
        "\n",
        "  if poetry_df['tokens'].str.contains(text, regex=False).any() == True :\n",
        "    row = poetry_df[poetry_df['tokens'].str.contains(text, regex=False)].values[0]\n",
        "    title, author, period, link = row[0], row[1], row[7], row[6]\n",
        "    return author, title, period, link\n",
        "  else:\n",
        "    return None, None, None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA3qC3py0uwf"
      },
      "source": [
        "df[['title', 'author', 'period', 'link']] = df.apply(find_original_poem, axis='columns', result_type='expand')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFlCB-aqy_h5"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu4SZ7R7U8Na"
      },
      "source": [
        "## **Plot word embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5PQYxa_ezps"
      },
      "source": [
        "Lastly, we will plot the words vectors from this DataFrame with the Python data viz library [Altair](https://altair-viz.github.io/gallery/scatter_tooltips.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwyogpt7zkOk"
      },
      "source": [
        "import altair as alt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE_kCW9QUDKX"
      },
      "source": [
        "alt.Chart(df,title=\"Word Similarity: Bank\").mark_circle(size=200).encode(\n",
        "    alt.X('x',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ), y=\"y\",\n",
        "    # If you click a point, take you to the URL link\n",
        "    href=\"link\",\n",
        "    # The categories that show up in the hover tooltip\n",
        "    tooltip=['title', 'context', 'author', 'period']\n",
        "    ).interactive().properties(\n",
        "    width=500,\n",
        "    height=500\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLSjJc8-IZJ_"
      },
      "source": [
        "## **Plot word embeddings from keywords (all at once!)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPXChKeD-Gd6"
      },
      "source": [
        "We can put the code from the previous few sections into a single cell and plot the BERT word embeddings for any list of words. Let's look at the words \"nature,\" \"religion,\" \"science,\" and \"art.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA-SdhjlprIj"
      },
      "source": [
        "# List of keywords that you want to compare\n",
        "keywords = ['nature', 'religion', 'science', 'art']\n",
        "\n",
        "# How to color the points in the plot. The other option is \"period\" for time period\n",
        "color_by = 'word'\n",
        "\n",
        "# Get all word positions\n",
        "word_positions = get_word_positions(keywords)\n",
        "\n",
        "# Get all contexts around the words\n",
        "keyword_contexts = []\n",
        "keyword_contexts_tokens = []\n",
        "words = []\n",
        "\n",
        "for position in word_positions:\n",
        "  words.append(word_lookup[all_word_ids[position]])\n",
        "  keyword_contexts.append(get_context_clean(position))\n",
        "  keyword_contexts_tokens.append(get_context(position))\n",
        "\n",
        "# Reduce word vectors with PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(all_word_vectors[word_positions,:].T)\n",
        "\n",
        "# Make a DataFrame with PCA results\n",
        "df = pd.DataFrame({\"x\": pca.components_[0,:], \"y\": pca.components_[1,:],\n",
        "                   \"context\": keyword_contexts, \"tokens\": keyword_contexts_tokens, \"word\": words})\n",
        "# Match original text and metadata\n",
        "df[['title', 'author', 'period', 'link']] = df.apply(find_original_poem, axis='columns', result_type='expand')\n",
        "\n",
        "# Rename columns so that the context shows up as the \"title\" in the tooltip (bigger and bolded)\n",
        "df = df.rename(columns={'title': 'poem_title', 'context': 'title'})\n",
        "\n",
        "# Make the plot\n",
        "alt.Chart(df, title=f\"Word Similarity: {', '.join(keywords).title()}\").mark_circle(size=200).encode(\n",
        "    alt.X('x',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ), y=\"y\",\n",
        "    color= color_by,\n",
        "    href=\"link\",\n",
        "    tooltip=['title', 'word', 'poem_title', 'author', 'period']\n",
        "    ).interactive().properties(\n",
        "    width=500,\n",
        "    height=500\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8t76MW9-ZY8"
      },
      "source": [
        "Let's examine the words \"nature,\" \"religion,\" \"science,\" and \"art\" again but this time color the points by their time period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYvTEC7_2jIr"
      },
      "source": [
        "# List of keywords that you want to compare\n",
        "keywords = ['nature', 'religion', 'science', 'art']\n",
        "\n",
        "# How to color the points in the plot\n",
        "color_by = 'period'\n",
        "\n",
        "# Get all word positions\n",
        "word_positions = get_word_positions(keywords)\n",
        "\n",
        "# Get all contexts around the words\n",
        "keyword_contexts = []\n",
        "keyword_contexts_tokens = []\n",
        "words = []\n",
        "\n",
        "for position in word_positions:\n",
        "  words.append(word_lookup[all_word_ids[position]])\n",
        "  keyword_contexts.append(get_context_clean(position))\n",
        "  keyword_contexts_tokens.append(get_context(position))\n",
        "\n",
        "# Reduce word vectors with PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(all_word_vectors[word_positions,:].T)\n",
        "\n",
        "# Make a DataFrame with PCA results\n",
        "df = pd.DataFrame({\"x\": pca.components_[0,:], \"y\": pca.components_[1,:],\n",
        "                   \"context\": keyword_contexts, \"tokens\": keyword_contexts_tokens, \"word\": words})\n",
        "# Match original text and metadata\n",
        "df[['title', 'author', 'period', 'link']] = df.apply(find_original_poem, axis='columns', result_type='expand')\n",
        "\n",
        "# Rename columns so that the context shows up as the \"title\" in the tooltip (bigger and bolded)\n",
        "df = df.rename(columns={'title': 'poem_title', 'context': 'title'})\n",
        "\n",
        "# Make the plot\n",
        "alt.Chart(df, title=f\"Word Similarity: {', '.join(keywords).title()}\").mark_circle(size=200).encode(\n",
        "    alt.X('x',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ), y=\"y\",\n",
        "    color= 'period',\n",
        "    href=\"link\",\n",
        "    tooltip=['title', 'word', 'poem_title', 'author', 'period']\n",
        "    ).interactive().properties(\n",
        "    width=500,\n",
        "    height=500\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FP132HwHQzU"
      },
      "source": [
        "Let's compare the words \"mean,\" \"thin,\" \"average\", and \"cruel.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyQ5IN6OF1gR"
      },
      "source": [
        "# List of keywords that you want to compare\n",
        "keywords = ['mean', 'thin', 'average', 'cruel']\n",
        "\n",
        "# How to color the points in the plot. The other option is \"period\" for time period\n",
        "color_by = 'word'\n",
        "\n",
        "# Get all word positions\n",
        "word_positions = get_word_positions(keywords)\n",
        "\n",
        "# Get all contexts around the words\n",
        "keyword_contexts = []\n",
        "keyword_contexts_tokens = []\n",
        "words = []\n",
        "\n",
        "for position in word_positions:\n",
        "  words.append(word_lookup[all_word_ids[position]])\n",
        "  keyword_contexts.append(get_context_clean(position))\n",
        "  keyword_contexts_tokens.append(get_context(position))\n",
        "\n",
        "# Reduce word vectors with PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(all_word_vectors[word_positions,:].T)\n",
        "\n",
        "# Make a DataFrame with PCA results\n",
        "df = pd.DataFrame({\"x\": pca.components_[0,:], \"y\": pca.components_[1,:],\n",
        "                   \"context\": keyword_contexts, \"tokens\": keyword_contexts_tokens, \"word\": words})\n",
        "# Match original text and metadata\n",
        "df[['title', 'author', 'period', 'link']] = df.apply(find_original_poem, axis='columns', result_type='expand')\n",
        "\n",
        "# Rename columns so that the context shows up as the \"title\" in the tooltip (bigger and bolded)\n",
        "df = df.rename(columns={'title': 'poem_title', 'context': 'title'})\n",
        "\n",
        "# Make the plot\n",
        "alt.Chart(df, title=f\"Word Similarity: {', '.join(keywords).title()}\").mark_circle(size=200).encode(\n",
        "    alt.X('x',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ), y=\"y\",\n",
        "    color= color_by,\n",
        "    href=\"link\",\n",
        "    tooltip=['title', 'word', 'poem_title', 'author', 'period']\n",
        "    ).interactive().properties(\n",
        "    width=500,\n",
        "    height=500\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSigZdkxHX2N"
      },
      "source": [
        "Let's compare the words 'head', 'heart', 'eye', 'arm', and 'leg.'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpAy-NUbGsca"
      },
      "source": [
        "# List of keywords that you want to compare\n",
        "keywords = ['head', 'heart', 'eye', 'arm', 'leg']\n",
        "\n",
        "# How to color the points in the plot. The other option is \"period\" for time period\n",
        "color_by = 'word'\n",
        "\n",
        "# Get all word positions\n",
        "word_positions = get_word_positions(keywords)\n",
        "\n",
        "# Get all contexts around the words\n",
        "keyword_contexts = []\n",
        "keyword_contexts_tokens = []\n",
        "words = []\n",
        "\n",
        "for position in word_positions:\n",
        "  words.append(word_lookup[all_word_ids[position]])\n",
        "  keyword_contexts.append(get_context_clean(position))\n",
        "  keyword_contexts_tokens.append(get_context(position))\n",
        "\n",
        "# Reduce word vectors with PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(all_word_vectors[word_positions,:].T)\n",
        "\n",
        "# Make a DataFrame with PCA results\n",
        "df = pd.DataFrame({\"x\": pca.components_[0,:], \"y\": pca.components_[1,:],\n",
        "                   \"context\": keyword_contexts, \"tokens\": keyword_contexts_tokens, \"word\": words})\n",
        "# Match original text and metadata\n",
        "df[['title', 'author', 'period', 'link']] = df.apply(find_original_poem, axis='columns', result_type='expand')\n",
        "\n",
        "# Rename columns so that the context shows up as the \"title\" in the tooltip (bigger and bolded)\n",
        "df = df.rename(columns={'title': 'poem_title', 'context': 'title'})\n",
        "\n",
        "# Make the plot\n",
        "alt.Chart(df, title=f\"Word Similarity: {', '.join(keywords).title()}\").mark_circle(size=200).encode(\n",
        "    alt.X('x',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ), y=\"y\",\n",
        "    color= color_by,\n",
        "    href=\"link\",\n",
        "    tooltip=['title', 'word', 'poem_title', 'author', 'period']\n",
        "    ).interactive().properties(\n",
        "    width=500,\n",
        "    height=500\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJH5Cmu9HjoA"
      },
      "source": [
        "# List of keywords that you want to compare\n",
        "keywords = ['ring', 'crown']\n",
        "\n",
        "# How to color the points in the plot. The other option is \"period\" for time period\n",
        "color_by = 'word'\n",
        "\n",
        "# Get all word positions\n",
        "word_positions = get_word_positions(keywords)\n",
        "\n",
        "# Get all contexts around the words\n",
        "keyword_contexts = []\n",
        "keyword_contexts_tokens = []\n",
        "words = []\n",
        "\n",
        "for position in word_positions:\n",
        "  words.append(word_lookup[all_word_ids[position]])\n",
        "  keyword_contexts.append(get_context_clean(position))\n",
        "  keyword_contexts_tokens.append(get_context(position))\n",
        "\n",
        "# Reduce word vectors with PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(all_word_vectors[word_positions,:].T)\n",
        "\n",
        "# Make a DataFrame with PCA results\n",
        "df = pd.DataFrame({\"x\": pca.components_[0,:], \"y\": pca.components_[1,:],\n",
        "                   \"context\": keyword_contexts, \"tokens\": keyword_contexts_tokens, \"word\": words})\n",
        "# Match original text and metadata\n",
        "df[['title', 'author', 'period', 'link']] = df.apply(find_original_poem, axis='columns', result_type='expand')\n",
        "\n",
        "# Rename columns so that the context shows up as the \"title\" in the tooltip (bigger and bolded)\n",
        "df = df.rename(columns={'title': 'poem_title', 'context': 'title'})\n",
        "\n",
        "# Make the plot\n",
        "alt.Chart(df, title=f\"Word Similarity: {', '.join(keywords).title()}\").mark_circle(size=200).encode(\n",
        "    alt.X('x',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ), y=\"y\",\n",
        "    color= color_by,\n",
        "    href=\"link\",\n",
        "    tooltip=['title', 'word', 'poem_title', 'author', 'period']\n",
        "    ).interactive().properties(\n",
        "    width=500,\n",
        "    height=500\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQb1GuEBrYsw"
      },
      "source": [
        "## **Go from PCA (or t-SNE) to \"sense clusters\"**\n",
        "\n",
        "[ Link to Lucy's slides ](https://docs.google.com/presentation/d/1yTupYj9g78XfbOqN9hSF33xdi5eiFgZZBVVK46SNKp8/edit#slide=id.gd6f9574c10_0_296)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyxVp-ZWrIM8"
      },
      "source": [
        "df.to_csv('bert-word-ring.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOrNI-8AdbHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This notebook draws upon materials created by the [The BERT for Humanists](https://melaniewalsh.github.io/BERT-for-Humanists/) Team, with modifications from Lauren Klein in 2022 and 2024*"
      ],
      "metadata": {
        "id": "Cfb7j20lde2f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3FlFrwgldf7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}