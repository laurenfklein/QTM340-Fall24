{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Your turn!\n",
        "\n",
        "Now it's your turn! Now we're going to put a few pieces together from various lessons in order to cluster the adjectives that people are using to describe food in their reviews. I'm going to set up a few things for you but you're going to do most of the work!\n"
      ],
      "metadata": {
        "id": "ukBUOS3EMPfW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB4EF4n5ZD2o"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from sklearn import feature_extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load Dataset\n",
        "\n",
        "First let's load in our data. Remember that this is a JSON file of food reviews with some metadata."
      ],
      "metadata": {
        "id": "eDahqp2hgcJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading large files from Google Drive\n",
        "import gdown\n",
        "\n",
        "# download the reviews w/ cuisine categories\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1WA_KAOXWOU8yyslDRtvl_JTl6Da0LniS', quiet=False)"
      ],
      "metadata": {
        "id": "xA7gNLycgbcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put it in a dataframe"
      ],
      "metadata": {
        "id": "m0p7ODXxNNEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the reviews json file\n",
        "reviews_df = pd.read_json(path_or_buf=\"./atl_reviews_with_cats.json\")\n",
        "\n",
        "reviews_df"
      ],
      "metadata": {
        "id": "UTMwKg2zNO9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a new dataframe that consists of a slice of the above with only the contents of `row['comment']['text']`"
      ],
      "metadata": {
        "id": "-ikaTC-RNaxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df = reviews_df['comment'].apply(lambda x: x['text'])\n",
        "\n",
        "comments_df"
      ],
      "metadata": {
        "id": "l1sqCJNFQe_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a random sample of 5000 reviews so that we finish this during the class time!"
      ],
      "metadata": {
        "id": "5CXdtbLvVCxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_sample_df = comments_df.sample(n=5000, random_state=1)\n",
        "comments_sample_df"
      ],
      "metadata": {
        "id": "nheppoZTVMhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Find the adjectives\n",
        "\n",
        "Your turn!\n",
        "\n",
        "The goal here is to create a list called `all_adj_list` that consists of all of the adjectives that appear in these reviews.\n",
        "\n",
        "In order to do this you will need to:\n",
        "1. Iterate through the `comment_df` dataframe\n",
        "2. Use spaCy to identify any adjectives in that comment\n",
        "3. Append each adjective to a list\n",
        "\n",
        "In the next step, we will sort the adjectives by frequency. But for now, we just want to pull the adjectives out of the comments."
      ],
      "metadata": {
        "id": "Fq1p61kpQ8-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# required imports and instantiation of the spacy model\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "# list for storing adjectives\n",
        "all_adj_list = []\n",
        "\n",
        "# the rest of your code here!\n",
        "for comment in comments_sample_df:\n",
        "    doc = nlp(comment)\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'ADJ':\n",
        "           # print(\"Adding: \" + str(token))\n",
        "            all_adj_list.append(str(token))\n",
        "\n",
        "len(all_adj_list)\n"
      ],
      "metadata": {
        "id": "f2iIyRobS48x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our list, here's some code to sort it by value counts. We're going to work with adjectives that only appear 5 times or more in our reviews dataset."
      ],
      "metadata": {
        "id": "9g4I5dDlXH1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# use Counter to create dict w/ value counts from list\n",
        "all_adj_dict = dict(Counter(all_adj_list))\n",
        "\n",
        "print(\"Number of unique adjectives: \" + str(len(all_adj_dict)))\n",
        "\n",
        "# now create a set w/ adjectives used 5 or more times\n",
        "adj_vocab = set()\n",
        "\n",
        "for adj in all_adj_dict:\n",
        "  if all_adj_dict[adj] > 4:\n",
        "    adj_vocab.add(adj)\n",
        "\n",
        "print(\"Number of adjectives used 5 or more times: \" + str(len(adj_vocab)))\n",
        "\n",
        "# take a look\n",
        "print(adj_vocab)"
      ],
      "metadata": {
        "id": "FPowAFcVTYLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Get BERT embeddings"
      ],
      "metadata": {
        "id": "QrCKJda5cqHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chunk of code comes from our word similarity notebook. I've pre-run it and saved the embeddings in a JSON file that we can load instead of running all of this. But it's good to look at so you can see how the parts are coming together.\n",
        "\n",
        "First, we tokenize our reviews for BERT."
      ],
      "metadata": {
        "id": "9hHuqpzvbIAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "comments_text = comments_sample_df.tolist()\n",
        "\n",
        "tokenized_comments = tokenizer(comments_text, truncation=True, padding=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "SX6I6ROJYtLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we load the pretrained model:"
      ],
      "metadata": {
        "id": "qCo4i0xjcAdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertModel\n",
        "\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(\"cuda\")"
      ],
      "metadata": {
        "id": "qISPqf1MfrjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we get the BERT embeddings for each of the adjectives in our `adj_vocab`.\n"
      ],
      "metadata": {
        "id": "EKBq2R1Wf4cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of vocabulary word IDs for all the words in each document (aka each review)\n",
        "doc_word_ids = []\n",
        "\n",
        "# List of word vectors for all the words in each document (aka each review)\n",
        "doc_word_vectors = []\n",
        "\n",
        "# Below we will slice our reviews to ignore the first (0th) and last (-1) special BERT tokens\n",
        "start_of_words = 1\n",
        "end_of_words = -1\n",
        "\n",
        "# Below we will index the 0th or first document, which will be the only document, since we're analzying one review at a time\n",
        "first_document = 0\n",
        "\n",
        "for i, review in enumerate(comments_text):\n",
        "\n",
        "    # Here we tokenize each poem with the DistilBERT Tokenizer\n",
        "    inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Here we extract the vocabulary word ids for all the words in the review (the first or 0th document, since we only have one document)\n",
        "    # We ignore the first and last special BERT tokens\n",
        "    # We also convert from a Pytorch tensor to a numpy array\n",
        "    doc_word_ids.append(inputs.input_ids[first_document].numpy()[start_of_words:end_of_words])\n",
        "\n",
        "    # Here we send the tokenized reviews to the GPU\n",
        "    # The model is already on the GPU, but this review isn't, so we send it to the GPU\n",
        "    inputs.to(\"cuda\")\n",
        "    # Here we run the tokenized reviews through the DistilBERT model\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # We take every element from the first or 0th document, from the 2nd to the 2nd to last position\n",
        "    # Grabbing the last layer is one way of getting token vectors. There are different ways to get vectors with different pros and cons\n",
        "    doc_word_vectors.append(outputs.last_hidden_state[first_document,start_of_words:end_of_words,:].detach().cpu().numpy())\n",
        "\n"
      ],
      "metadata": {
        "id": "fjHI7cgSf2ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenate all wordIDs/vectors for all documents"
      ],
      "metadata": {
        "id": "47YqoX0zheRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_word_ids = np.concatenate(doc_word_ids)\n",
        "all_word_vectors = np.concatenate(doc_word_vectors, axis=0)"
      ],
      "metadata": {
        "id": "unK4G4jnhltC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now pull out the vectors assocaited with the adjectives in our `adj_vocab` set"
      ],
      "metadata": {
        "id": "G3R-0jhejCbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# newer version -- more error checking, create average vectors\n",
        "adj_vocab_list = list(adj_vocab)\n",
        "\n",
        "final_vocab_list = []\n",
        "avg_adj_vectors = []\n",
        "\n",
        "for adj in adj_vocab_list:\n",
        "  if adj in tokenizer.vocab:\n",
        "    final_vocab_list.append(adj)\n",
        "\n",
        "    adj_vectors = []\n",
        "\n",
        "    # get word_id\n",
        "    word_id = tokenizer.vocab[adj]\n",
        "\n",
        "    # find all the positions where the words occur in the dataset\n",
        "    word_positions = np.where(np.isin(all_word_ids, word_id))\n",
        "\n",
        "    # get the vectors for all those posiitons\n",
        "    adj_vectors.append(np.mean(all_word_vectors[word_positions], axis=0))\n",
        "\n",
        "    # create average vector for all vectors in adj_vectors\n",
        "    average_adj_vector = np.mean(adj_vectors, axis=0)\n",
        "\n",
        "    # append to avg_adj_vectors\n",
        "    avg_adj_vectors.append(average_adj_vector)\n",
        "\n",
        "# len should be same or less than number of original adjecives\n",
        "# due to not in vocab vectors\n",
        "len(avg_adj_vectors)"
      ],
      "metadata": {
        "id": "MkCKGlFGi2oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(avg_adj_vectors)"
      ],
      "metadata": {
        "id": "J-2E_uNPplY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_vocab_list)"
      ],
      "metadata": {
        "id": "G5DLshZ1oVVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NcrKr1aoptvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export adj_vectors as pickle file\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('avg_adj_vectors_596.pkl', 'wb') as f:\n",
        "    pickle.dump(avg_adj_vectors, f)\n",
        "\n",
        "with open('final_vocab_list_596.pkl', 'wb') as f:\n",
        "    pickle.dump(final_vocab_list, f)"
      ],
      "metadata": {
        "id": "hCvnYhyGsQYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Cluster adjective vectors!"
      ],
      "metadata": {
        "id": "ioZXkrR3k2xR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the BERT vectors if you couldn't run the code above"
      ],
      "metadata": {
        "id": "LR4vdXFDnU8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import pickle\n",
        "\n",
        "# import pickle files\n",
        "# vectors\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1itJrQVYCrCCeol5eWZmC5-jaIbWSzKfK', quiet=False)\n",
        "\n",
        "# vocab\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1P1z7wzm4IEsGn3dpkb4MrvYX6a0I1PbP', quiet=False)\n",
        "\n",
        "with open('avg_adj_vectors_596.pkl', 'rb') as f:\n",
        "  avg_adj_vectors = pickle.load(f)\n",
        "\n",
        "with open('final_vocab_list_596.pkl', 'rb') as f:\n",
        "  final_vocab_list = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "_aJdQdGgh63d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with two columns, final_vocab_list and avg_adj_vectors\n",
        "\n",
        "adj_vectors_df = pd.DataFrame({'word': final_vocab_list, 'BERT_vector': avg_adj_vectors})\n",
        "\n",
        "adj_vectors_df"
      ],
      "metadata": {
        "id": "8DHKo1genS8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4a. Determine best K\n",
        "\n",
        "Since we have ~600 vectors, let's try k up to 30"
      ],
      "metadata": {
        "id": "ppVkOjBNc9AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "inertia_vals = []\n",
        "\n",
        "for i in range(1, 30):\n",
        "    km_clustering = KMeans(n_clusters=i, n_init=10)\n",
        "    km_clustering.fit(avg_adj_vectors)\n",
        "    inertia_vals.append(km_clustering.inertia_)\n",
        "\n",
        "ks = list(range(1, 30))"
      ],
      "metadata": {
        "id": "BHNKfe9is4az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# library for quickly calculating \"knee\"\n",
        "%pip install kneed"
      ],
      "metadata": {
        "id": "1Mw3pbtjdNBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine knee\n",
        "from kneed import KneeLocator\n",
        "kn = KneeLocator(ks, inertia_vals, curve='convex', direction='decreasing')\n",
        "print(kn.knee)"
      ],
      "metadata": {
        "id": "hHc2cXTidkmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of clusters k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.plot(ks, inertia_vals, 'bx-')\n",
        "plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')"
      ],
      "metadata": {
        "id": "GhGExJJHdn87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4b. Cluster!\n"
      ],
      "metadata": {
        "id": "2Mjwy3pudvFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 8\n",
        "\n",
        "km = KMeans(n_clusters=num_clusters, n_init=10) # default is also 10, but good to know\n",
        "\n",
        "clusters = km.fit_predict(avg_adj_vectors)\n",
        "\n",
        "adj_vectors_df['cluster'] = clusters\n",
        "\n",
        "adj_vectors_df"
      ],
      "metadata": {
        "id": "o47Kxy7TnNuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# find all adjectives in each cluster\n",
        "for i in range(num_clusters):\n",
        "    print(\"Adjectives in cluster \" + str(i) + \": \")\n",
        "    cluster_adjs = \"\"\n",
        "\n",
        "    # create new df of only the specific cluster\n",
        "    # remember boolean selection!\n",
        "    cluster_df = adj_vectors_df[ adj_vectors_df[\"cluster\"] == i ]\n",
        "\n",
        "    # create series of adjectives assoc w/ that cluster\n",
        "    for adj in cluster_df['word']:\n",
        "        cluster_adjs += adj + \", \"\n",
        "\n",
        "    print(textwrap.fill(cluster_adjs, 100) + \"\\n\")"
      ],
      "metadata": {
        "id": "drFzFx2trzeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you had to provide an overal characterization of each of these clusters, what would you say?**"
      ],
      "metadata": {
        "id": "uuww__4HshZo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2A3hp4eKsqEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5PRsw2ssp_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4mvxuq6sp47"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}