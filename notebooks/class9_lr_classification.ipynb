{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2N1ZLUCMyQa"
      },
      "source": [
        "# LR Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thkyRj39MyQd"
      },
      "source": [
        "Classification, a task popular in machine learning, determines whether and how a model can distinguish between sets of text.\n",
        "\n",
        "It works like this. Everyone with email relies on classification to separate spam from legitimate emails. Email providers train classification models to recognize the difference by giving them emails they have labeled “spam” and “not spam.” They then ask the model to learn the features that most reliably distinguish the two types, which could include a preponderance of all caps or phrases like “free money” or “get paid.” They test the model by giving it unlabeled emails and asking it to classify them. If the model can do it accurately a high percentage of the time, that’s a good spam filter.\n",
        "\n",
        "We can take the underlying idea and apply it to many experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JwWzSwnMyQe"
      },
      "source": [
        "## Today's experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_2URE2jMyQe"
      },
      "source": [
        "We are going to use a corpus of obituaries from _The New York Times_ in order to test whether our model can learn to distinguish between obituaries about men and women."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64iAn2sLMyQf"
      },
      "source": [
        "## Imports\n",
        "\n",
        "As always, we begin with some imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSJJN37eMyQf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from pandas import DataFrame\n",
        "from pandas import Series, DataFrame\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.stats import pearsonr, norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDKkosNuMyQg"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfXOjwXyMyQh"
      },
      "source": [
        "For this notebook, we'll be using a dataset of _New York Times_ obituaries. Let's import the gdown library so that we can load in the dataset and unzip it:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading large files from Google Drive\n",
        "import gdown\n",
        "\n",
        "# Download the zip file\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1G0Aeg8dzZGPOCNFZ77U-s9CfEnR8efbB', quiet=False)"
      ],
      "metadata": {
        "id": "1oON3SrpOTzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip it\n",
        "!unzip NYT-Obituaries.zip"
      ],
      "metadata": {
        "id": "mY_g1gAyO6zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcOv-xMZMyQh"
      },
      "outputs": [],
      "source": [
        "# collect filepaths as files\n",
        "directory = \"./NYT-Obituaries/\"\n",
        "files = glob.glob(f\"{directory}/*.txt\")\n",
        "\n",
        "len(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kK-o6NoMyQi"
      },
      "outputs": [],
      "source": [
        "# and collect obit titles, which are also the final section of the filepaths\n",
        "obit_titles = [Path(file).stem for file in files]\n",
        "obit_titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLJ2Uk2bMyQi"
      },
      "source": [
        "## Create document-term matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM2H7u6oMyQj"
      },
      "source": [
        "### Initiate CountVectorizer as vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiK6x4y9MyQj"
      },
      "source": [
        "Remember document-term matrices, aka doc-term matrices, aka dtms? We learned about them in our tf-idf notebooks. Our classifier uses a dtm as its input. We build it with scikit-learn's CountVectorizer, which we already imported up above.\n",
        "\n",
        "When we load our vectorizer, we include an argument to encode as utf-8 and we load our stopwords. In this case, we're using a custom stopwords list rather than the default sklearn one. You may end up using a custom stopwords list in your final projects.\n",
        "\n",
        "In addition, we can set the minimum number of times a word must appear in the corpus for it to be included in the dtm. In this case, I've set it at 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb5ixhCAMyQj"
      },
      "outputs": [],
      "source": [
        "# another sklearn library to help load stopwords\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "# Download the stopwords file\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1BQ8zVSiG_WKpXNB81Y1P9yyi2L43UeJD', quiet=False)\n",
        "\n",
        "# open it\n",
        "text_file = open('./jockers_stopwords.txt')\n",
        "jockers_words = text_file.read().split()\n",
        "new_stopwords = list(text.ENGLISH_STOP_WORDS.union(jockers_words))\n",
        "\n",
        "# create dtm\n",
        "corpus_path = './NYT-Obituaries/'\n",
        "vectorizer = CountVectorizer(input='filename',\n",
        "                             encoding='utf8',\n",
        "                             stop_words = new_stopwords,\n",
        "                             min_df=20, dtype='float64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hpg5fHzMyQk"
      },
      "source": [
        "### Make list of filepaths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HpIit-FMyQk"
      },
      "source": [
        "If you recall, CountVectorizer builds a dtm from a list of filepaths. So we will provide that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWrj3EY4MyQk"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "for title in obit_titles:\n",
        "    filename = title + \".txt\"\n",
        "    corpus.append(corpus_path + filename)\n",
        "dtm = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvHgq_OYMyQk"
      },
      "source": [
        "### Get feature names and set as column titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebq6_IHkMyQl"
      },
      "source": [
        "The columns store word counts. We want to name the columns with the words stored in each, and to transform the dtm into a pandas dataframe, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S8nW0-oMyQl"
      },
      "outputs": [],
      "source": [
        "vocab = vectorizer.get_feature_names_out()\n",
        "matrix = dtm.toarray()\n",
        "df = DataFrame(matrix, columns=vocab)\n",
        "print('df shape is: ' + str(df.shape))\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zaBoaGcMyQl"
      },
      "source": [
        "Our dataframee has 378 rows, one for each document, or obituary, and 2985 columns, one for each word that's not in stopwords and appears at least 20 times in the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv8svLFEMyQl"
      },
      "source": [
        "### Pandas interlude ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jObwxn2UMyQl"
      },
      "source": [
        "I re-found [this](https://twitter.com/mmitchell_ai/status/1454931443386228751) on Twitter the other night, posted by Dr. Margaret Mitchell, the former co-lead of Google's EthicalAI group and now Big Deal at HuggingFace (the folx behind the transformer libraries we'll be using next week):\n",
        "\n",
        "<img src=\"http://lklein.com/wp-content/uploads/2021/11/Screen-Shot-2021-11-01-at-10.14.24-AM.png\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lhoTQ2zMyQm"
      },
      "source": [
        "In any case, now it's time to import our metadata:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo8gFjIMMyQm"
      },
      "source": [
        "## Import metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWOEvp-zMyQm"
      },
      "outputs": [],
      "source": [
        "gdown.download('https://drive.google.com/uc?export=download&id=1Pca0n-vWTy_FcF0oKWt9iHwxuBrDnxfd', quiet=False)\n",
        "\n",
        "meta = pd.read_csv(\"./NYT-Obituaries.csv\", encoding = 'utf-8')\n",
        "meta = meta.rename(columns={'title': 'obit_title'})\n",
        "meta = meta[[\"obit_title\", \"gender\", \"date\"]]\n",
        "meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYtqyBnlMyQm"
      },
      "source": [
        "Our metadata is stored as a pandas dataframe with a row for each obituary and three columns: title, gender, and year.\n",
        "\n",
        "Let's now concatinate the dtm to it so that everything is in one place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF3xLu-sMyQm"
      },
      "source": [
        "## Concatenate metadata and doc-term dataframe\n",
        "\n",
        "We'll use the pandas `concat` methods, specifying that the data should be concatinated as additional columns (that's the `axis = 1` parameter. (The default would be `0` to concatinate as additional rows.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRkWETjgMyQm"
      },
      "outputs": [],
      "source": [
        "df_concat = pd.concat([meta, df], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlligcekMyQm"
      },
      "outputs": [],
      "source": [
        "df_concat.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjdTSL38MyQm"
      },
      "source": [
        "## Equalize numbers of men and women"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5K9dMgrMyQn"
      },
      "source": [
        "We want our dataframe to have equal numbers of men and women. How many women are there? Women are counted as 1 and men as 0, so if we sum the gender column, we'll have the number of women:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrzPwbpaMyQn"
      },
      "outputs": [],
      "source": [
        "meta['gender'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO3r8chHMyQn"
      },
      "source": [
        "Then we separate men and women into two dataframes and take a random sample of 93 obituaries about men."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzU5v09CMyQn"
      },
      "outputs": [],
      "source": [
        "df_men = df_concat[df_concat['gender'] == 0]\n",
        "df_women = df_concat[df_concat['gender'] == 1]\n",
        "df_men = df_men.sample(n=93)\n",
        "\n",
        "df_men"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfg8NOVWMyQn"
      },
      "source": [
        "We then concatenate the sampled men dataframe with the women dataframe and reset the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvhuWj9VMyQn"
      },
      "outputs": [],
      "source": [
        "df_final = pd.concat([df_men, df_women])\n",
        "df_final = df_final.reset_index()\n",
        "df_final = df_final.drop(columns=\"index\")\n",
        "df_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBUD2m_8MyQn"
      },
      "source": [
        "We now have 186 rows: 93 men, 93 women."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jErtMyTgMyQo"
      },
      "source": [
        "### Match meta and data dataframes with subset of df_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teb5WqNHMyQo"
      },
      "source": [
        "We'll continue to use meta and df, so we need to ensure they match our subsetted df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRgiezEIMyQo"
      },
      "outputs": [],
      "source": [
        "meta = df_final[[\"obit_title\", \"gender\", \"date\"]]\n",
        "meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrCMOuC3MyQo"
      },
      "outputs": [],
      "source": [
        "df = df_final.loc[:,'000':]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le-nksyzMyQo"
      },
      "source": [
        "## Let's run our classifier!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF3jMAmkMyQo"
      },
      "source": [
        "Once we have a dataframe with metadata and vocab counts we're ready to run our classifier!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0y_gS7JMyQo"
      },
      "source": [
        "### We add columns for probabilities and predicted class to our metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LQhLr2gMyQo"
      },
      "source": [
        "As we run the model, we are going to store its output with our metadata. This will allow us to easily examine the model's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g__-yBjbMyQo"
      },
      "outputs": [],
      "source": [
        "meta['PROBS'] = ''\n",
        "meta['PREDICTED'] = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_IHinhIMyQp"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSp_jqk9MyQp"
      },
      "source": [
        "We will use scikit-learn's `LogisticRegression` model. There are many other options for classifier models. Some are better for some tasks, other for others. LogisticRegression is standard for classifying literature. We set the penalty as L1 (Lasso / least absolute deviations) and the 'C' value as 1.0.\n",
        "\n",
        "If you decide to specialize in classification, you can explore further the implications of these regularization parameters. If you're curious now, [here is a nice intuitive primer](https://medium.com/analytics-vidhya/l1-vs-l2-regularization-which-is-better-d01068e6658c) on L1 vs the other main option, L2 (Ridge Regression). (In sk-learn, there is also an option in the middle, the Elastic-Net, which leads to an amount of sparsity between that of L1 and L2.)\n",
        "\n",
        "As far as C, large values of C (like ours) give more freedom to the model. Conversely, smaller values of C (e.g. .1 or .01) constrain the model more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrQ3Gan5MyQp"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(penalty = 'l1', C = 1.0, solver='liblinear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wbx-G9sMyQp"
      },
      "source": [
        "### Run the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1fQ90zUMyQp"
      },
      "source": [
        "We run the model in the following for-loop.\n",
        "\n",
        "Classification models need classes: they need the texts grouped into different sets. Our metadata has built-in classes: gender. Men are stored as 0; women as 1. We could, if we wanted, create a new 0/1 class based on something else.\n",
        "\n",
        "Each iteration trains on all the titles except one, then predicts which class the excluded title belongs to. We'll call this leave-one-out classification. It's helpful when you're working with a small dataset. There are other ways of dividing training and testing sets, including the 80/20 split we used lass class with our book reviews genres classifier.\n",
        "\n",
        "Here, the first four indented lines simply track our progress by printing index, title, and class. The next four lines exclude a single title, and set the training data and the test data.\n",
        "\n",
        "The final six lines fit the model, calculate the probabilities and predicted class of the test case, and add that information to our metadata dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYSomYcmMyQp"
      },
      "outputs": [],
      "source": [
        "for this_index in df_final.index.tolist():\n",
        "    print(this_index) # keep track of where we are in the corpus\n",
        "    title = meta.loc[meta.index[this_index], 'obit_title']\n",
        "    CLASS = meta.loc[meta.index[this_index], 'gender']\n",
        "    print(title, CLASS)\n",
        "\n",
        "    train_index_list = [index_ for index_ in df.index.tolist() if index_ != this_index] # exclude the title to be predicted\n",
        "    X = df.loc[train_index_list] # the model trains on all the data except the excluded title row\n",
        "    y = meta.loc[train_index_list, 'gender'] # the y row tells the model which class each title belongs to\n",
        "    TEST_CASE = df.loc[[this_index]]\n",
        "\n",
        "    model.fit(X,y) # fit the model\n",
        "    prediction = model.predict_proba(TEST_CASE) # calculate probability of test case\n",
        "    predicted = model.predict(TEST_CASE) # calculate predicted class of test case\n",
        "    meta.at[this_index, 'PREDICTED'] = predicted # add predicted class to metadata\n",
        "    meta.at[this_index, 'PROBS'] = str(prediction) # add probabilities to metadata\n",
        "    print('Class is: ' + str(CLASS) + '\\n' + 'Prediction is: ' + str(predicted) + ' ' + str(prediction) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODSXC1FQMyQq"
      },
      "source": [
        "How cool is this! For each obituary, we see who it's about, that person's gender (0 or 1), and which gender the model thinks it's about, by which probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYyHnXYCMyQq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbNaV3BiMyQq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZnQB2D0MyQq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgAKt0YlMyQq"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dLjIWa8MyQq"
      },
      "source": [
        "Remember, we've stored our results in our metadata dataframe. Let's take a look!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBjTOhkYMyQq"
      },
      "outputs": [],
      "source": [
        "meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzjnVm1zMyQq"
      },
      "source": [
        "There's lots to look at here. We could explore probabilities: which obituaries is the model most sure about? Which are closest to 50-50? Which does it get most right and most wrong? Is there a pattern to misclassified obituaries?\n",
        "\n",
        "For now, we just want to calculate its accuracy. Let's get rid of those brackets in the PREDICTED column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2Ep3UrEMyQq"
      },
      "outputs": [],
      "source": [
        "meta = meta.replace([0], 0)\n",
        "meta = meta.replace([1], 1)\n",
        "meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZfS0GKEMyQr"
      },
      "source": [
        "### Result column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywh8v2m3MyQr"
      },
      "source": [
        "Now we can add a 'RESULT' column that is the result of subtracting the predicted gender from the actual gender.\n",
        "\n",
        "0 means the model was correct.\n",
        "-1 means the model mistook a man for a woman.\n",
        "1 means the model mistook a woman for a man."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVJU-3y-MyQr"
      },
      "outputs": [],
      "source": [
        "sum_column = meta['gender'] - meta['PREDICTED']\n",
        "meta['RESULT'] = sum_column\n",
        "meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOAamlthMyQr"
      },
      "source": [
        "Let's look at the accurate guesses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jAahnmLMyQr"
      },
      "outputs": [],
      "source": [
        "# note that we're not wanting to rewrite the \"meta\" df here, just look at it\n",
        "# so we won't reassign it\n",
        "meta[meta['RESULT'] == 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4JQQKcyMyQr"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7mTSrmjMyQr"
      },
      "source": [
        "How many did the model get correct?\n",
        "\n",
        "We can calculate its accuracy by dividing the correct number by the total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEuxlyP0MyQr"
      },
      "outputs": [],
      "source": [
        "# remember our filter approach from last week's pandas class\n",
        "accuracy_filter =  meta['RESULT'] == 0\n",
        "\n",
        "# apply the filter\n",
        "accurate_results = meta[accuracy_filter]\n",
        "\n",
        "# do our division\n",
        "# hint: you can just use the len() method to give you the length of a dataframe\n",
        "\n",
        "len(accurate_results) / len(meta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAMT_jcuMyQs"
      },
      "source": [
        "Hmm... At random, the model should guess correctly 50% of the time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io9jX2ViMyQs"
      },
      "source": [
        "## BONUS - Plotting a confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYXzKL3PMyQs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zJl-QyOMyQs"
      },
      "outputs": [],
      "source": [
        "actual = meta['gender'].array   # remember that pandas columns are Series objects, so we need to convert them\n",
        "predicted = meta['PREDICTED'].array\n",
        "\n",
        "cm = confusion_matrix(actual, predicted)\n",
        "\n",
        "print(cm)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['male','female'])\n",
        "\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUuFBkJEMyQs"
      },
      "source": [
        "**What is this showing us?**\n",
        "\n",
        "That's all for now!\n",
        "\n",
        "In the next lesson, we'll explore _how_ the model made its calculations by learning which words matter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIIHzCoCMyQs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}